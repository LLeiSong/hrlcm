{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/hrlcm/hrlcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from augmentation import *\n",
    "from dataset import *\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle as pkl\n",
    "from models.deeplab import DeepLab\n",
    "from models.unet import UNet\n",
    "from train import Trainer\n",
    "from loss import BalancedCrossEntropyLoss\n",
    "import torch_optimizer as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/ubuntu/hrlcm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummy inline arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy args for testing\n",
    "class args_dummy:\n",
    "    def __init__(self):\n",
    "        self.exp_name = 'unet_norm_adabound_compose_lr'\n",
    "        self.data_dir = 'results/north'\n",
    "        self.out_dir = 'results/dl'\n",
    "        self.lowest_score = 10\n",
    "        self.noise_ratio = 0\n",
    "        self.trans_prob = 0.5\n",
    "        self.label_offset = 1\n",
    "        self.rg_rotate = '-90, 90'\n",
    "        self.model = 'unet'\n",
    "        self.train_mode = 'single'\n",
    "        self.out_stride = 8\n",
    "        self.gpu_devices = '0, 1, 2, 3'\n",
    "        self.sync_norm = False\n",
    "        self.max_lr = 0.001\n",
    "        self.base_lr = 0.001\n",
    "        self.clr_gamma = 0.9999\n",
    "        self.save_freq = 10\n",
    "        self.log_feq = 10\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 200\n",
    "        self.optimizer_name = 'AdamP'\n",
    "        self.resume = None\n",
    "        self.checkpoint_dir = os.path.join(self.out_dir, self.exp_name, 'checkpoints')\n",
    "        self.logs_dir = os.path.join(self.out_dir, self.exp_name, 'logs')\n",
    "\n",
    "\n",
    "# Initialize dummy args\n",
    "args = args_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory for saving files\n",
    "if args.exp_name:\n",
    "    args.checkpoint_dir = os.path.join(args.out_dir, args.exp_name, 'checkpoints')\n",
    "    args.logs_dir = os.path.join(args.out_dir, args.exp_name, 'logs')\n",
    "else:\n",
    "    args.checkpoint_dir = os.path.join(args.out_dir, args.model, 'checkpoints')\n",
    "    args.logs_dir = os.path.join(args.out_dir, args.model, 'logs')\n",
    "\n",
    "# Create dirs if necessary\n",
    "if not os.path.isdir(args.checkpoint_dir):\n",
    "    os.makedirs(args.checkpoint_dir)\n",
    "if not os.path.isdir(args.logs_dir):\n",
    "    os.makedirs(args.logs_dir)\n",
    "\n",
    "# Dir for mean and sd pickles\n",
    "args.stats_dir = os.path.join(args.data_dir, 'norm_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flags for GPU processing if available\n",
    "if torch.cuda.is_available():\n",
    "    args.use_gpu = True\n",
    "else:\n",
    "    args.use_gpu = False\n",
    "\n",
    "# Load dataset\n",
    "# Define rotate degrees\n",
    "args.rg_rotate = tuple(float(each) for each in args.rg_rotate.split(','))\n",
    "\n",
    "# synchronize transform for train dataset\n",
    "sync_transform = Compose([\n",
    "#     RandomScale(prob=args.trans_prob),\n",
    "    RandomFlip(prob=args.trans_prob),\n",
    "#     RandomCenterRotate(degree=args.rg_rotate,\n",
    "#                        prob=args.trans_prob),\n",
    "    SyncToTensor()\n",
    "])\n",
    "\n",
    "# synchronize transform for validate dataset\n",
    "val_transform = Compose([\n",
    "    SyncToTensor()\n",
    "])\n",
    "\n",
    "# Image transform\n",
    "# Load mean and sd for normalization\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"means.pkl\"), \"rb\") as input_file:\n",
    "    mean = tuple(pkl.load(input_file))\n",
    "\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"stds.pkl\"), \"rb\") as input_file:\n",
    "    std = tuple(pkl.load(input_file))\n",
    "img_transform = ImgNorm(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train dataset\n",
    "train_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                         usage='train',\n",
    "                         lowest_score=args.lowest_score,\n",
    "                         noise_ratio=args.noise_ratio,\n",
    "                         label_offset=args.label_offset,\n",
    "                         sync_transform=sync_transform,\n",
    "                         img_transform=img_transform,\n",
    "                         label_transform=None)\n",
    "# Put into DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=args.batch_size,\n",
    "                          num_workers=32,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "# Get validate dataset\n",
    "validate_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                            usage='validate',\n",
    "                            label_offset=args.label_offset,\n",
    "                            sync_transform=val_transform,\n",
    "                            img_transform=img_transform,\n",
    "                            label_transform=None)\n",
    "# Put into DataLoader\n",
    "validate_loader = DataLoader(dataset=validate_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             num_workers=32,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up network\n",
    "args.n_classes = train_dataset.n_classes\n",
    "args.n_channels = train_dataset.n_channels\n",
    "if args.model == \"deeplab\":\n",
    "    model = DeepLab(num_classes=args.n_classes,\n",
    "                    backbone='resnet',\n",
    "                    pretrained_backbone=False,\n",
    "                    output_stride=args.out_stride,\n",
    "                    sync_bn=False,\n",
    "                    freeze_bn=False,\n",
    "                    n_in=args.n_channels)\n",
    "else:\n",
    "    model = UNet(n_classes=args.n_classes,\n",
    "                 n_channels=args.n_channels)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "    # Get devices\n",
    "if args.gpu_devices:\n",
    "    args.gpu_devices = [int(each) for each in args.gpu_devices.split(',')]\n",
    "\n",
    "# Set model\n",
    "if args.use_gpu:\n",
    "    if args.gpu_devices:\n",
    "        torch.cuda.set_device(args.gpu_devices[0])\n",
    "        model = torch.nn.DataParallel(model, device_ids=args.gpu_devices)\n",
    "        if args.sync_norm:\n",
    "            model = convert_model(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = BalancedCrossEntropyLoss()\n",
    "\n",
    "# Set up tensorboard logging\n",
    "writer = SummaryWriter(log_dir=args.logs_dir)\n",
    "\n",
    "# Save config\n",
    "pkl.dump(args, open(os.path.join(args.checkpoint_dir, \"args.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "# if args.optimizer_name == 'Adadelta':\n",
    "#     optimizer = torch.optim.Adadelta(model.parameters(),\n",
    "#                                      lr=args.max_lr)\n",
    "# elif args.optimizer_name == 'Adam':\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# elif args.optimizer_name == 'AdamW':\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# else:\n",
    "#     print('Not supported optimizer, use Adam instead.')\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "optimizer = optim.AdaBound(model.parameters(), lr=0.001, final_lr=0.01, amsbound=True)\n",
    "# optimizer = optim.AdamP(model.parameters(), nesterov=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With constant learning rate\n",
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.set_description(\"[Epoch] lr: {:.4f}\".format(\n",
    "                round(optimizer.param_groups[0][\"lr\"], 4)))\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With learning rate scheduler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "# if args.optimizer_name == 'Adadelta':\n",
    "#     optimizer = torch.optim.Adadelta(model.parameters(),\n",
    "#                                      lr=args.max_lr)\n",
    "# elif args.optimizer_name == 'Adam':\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# elif args.optimizer_name == 'AdamW':\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# else:\n",
    "#     print('Not supported optimizer, use Adam instead.')\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# optimizer = optim.AdaBound(model.parameters(), lr=0.001, final_lr=0.01, amsbound=True)\n",
    "optimizer = optim.AdamP(model.parameters(), nesterov=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.set_description(\"[Epoch] lr: {:.4f}\".format(\n",
    "                round(optimizer.param_groups[0][\"lr\"], 4)))\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composed scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate curve\n",
    "import matplotlib.pyplot as plt\n",
    "lr_scheduler_1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.001)\n",
    "lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.005, \n",
    "                                                   step_size_up=1, step_size_down=3,\n",
    "                                                   gamma=0.988, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "\n",
    "lrs = []\n",
    "\n",
    "for i in range(200):\n",
    "    optimizer.step()\n",
    "    if i <= lr_scheduler_1.T_max:\n",
    "        lr_scheduler_1.step()\n",
    "    else:\n",
    "        lr_scheduler_2.step()\n",
    "    lrs.append(\n",
    "        optimizer.param_groups[0][\"lr\"]\n",
    "    )\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.001)\n",
    "lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.05, \n",
    "                                                   step_size_up=1, step_size_down=3,\n",
    "                                                   gamma=0.96, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "    \n",
    "    # Update learning rate\n",
    "    if epoch <= lr_scheduler_1.T_max:\n",
    "        lr_scheduler_1.step()\n",
    "    else:\n",
    "        lr_scheduler_2.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.set_description(\"[Epoch] lr: {:.4f}\".format(\n",
    "                round(optimizer.param_groups[0][\"lr\"], 4)))\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composed optimizer and scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.AdaBound(model.parameters(), lr=0.001, final_lr=0.01, amsbound=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lrs = []\n",
    "\n",
    "for i in range(200):\n",
    "    optimizer.step()\n",
    "    if i <= 30:\n",
    "        if i == 30:\n",
    "            lr_scheduler_1 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0008, max_lr=0.0012, \n",
    "                                                   step_size_up=1, step_size_down=3,\n",
    "                                                   gamma=0.97, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "    elif i <= 120:\n",
    "        lr_scheduler_1.step()\n",
    "        if i == 120:\n",
    "            lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0004, max_lr=0.0006, \n",
    "                                                   step_size_up=1, step_size_down=5,\n",
    "                                                   gamma=0.94, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "    else:\n",
    "        lr_scheduler_2.step()\n",
    "    lrs.append(\n",
    "        optimizer.param_groups[0][\"lr\"]\n",
    "    )\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.AdaBound(model.parameters(), lr=0.001, final_lr=0.01, amsbound=True)\n",
    "\n",
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "    \n",
    "    # Update learning rate\n",
    "    if epoch <= 30:\n",
    "        if epoch == 30:\n",
    "            lr_scheduler_1 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0008, max_lr=0.0012, \n",
    "                                                   step_size_up=1, step_size_down=3,\n",
    "                                                   gamma=0.97, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "    elif epoch > 30 and epoch <= 120:\n",
    "        lr_scheduler_1.step()\n",
    "        if epoch == 120:\n",
    "            lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0004, max_lr=0.0006, \n",
    "                                                   step_size_up=1, step_size_down=5,\n",
    "                                                   gamma=0.94, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "    else:\n",
    "        lr_scheduler_2.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.set_description(\"[Epoch] lr: {:.4f}\".format(\n",
    "                round(optimizer.param_groups[0][\"lr\"], 4)))\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and load packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from augmentation import *\n",
    "from dataset import *\n",
    "from metrics import ConfMatrix\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle as pkl\n",
    "from models.deeplab import DeepLab\n",
    "from models.unet import UNet\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, \\\n",
    "    fbeta_score, classification_report, hamming_loss\n",
    "\n",
    "class Precision_score(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        weighted_prec = precision_score(true_labels, predict_labels, average='weighted')\n",
    "\n",
    "        return weighted_prec\n",
    "\n",
    "\n",
    "class Recall_score(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        weighted_rec = recall_score(true_labels, predict_labels, average='weighted')\n",
    "\n",
    "        return weighted_rec\n",
    "\n",
    "\n",
    "class F1_score(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        weighted_f1 = f1_score(true_labels, predict_labels, average=\"weighted\")\n",
    "\n",
    "        return weighted_f1\n",
    "\n",
    "\n",
    "class F2_score(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        weighted_f2 = fbeta_score(true_labels, predict_labels, beta=2, average=\"weighted\")\n",
    "\n",
    "        return weighted_f2\n",
    "\n",
    "\n",
    "class Hamming_loss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        return hamming_loss(true_labels, predict_labels)\n",
    "\n",
    "\n",
    "class cls_report(nn.Module):\n",
    "    def __init__(self, target_names):\n",
    "        super().__init__()\n",
    "        self.target_names = target_names\n",
    "\n",
    "    def forward(self, predict_labels, true_labels):\n",
    "        report = classification_report(true_labels, predict_labels,\n",
    "                                       target_names=self.target_names,\n",
    "                                       output_dict=True)\n",
    "\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eval_args_dummy:\n",
    "    def __init__(self):\n",
    "        self.args_path = 'results/dl/unet_norm_adabound_compose_lr/checkpoints/args.pkl'\n",
    "        self.checkpoint_file = 'results/dl/unet_norm_adabound_compose_lr/checkpoints/final.pth'\n",
    "        self.stats_dir = 'results/north/norm_stats'\n",
    "        self.data_dir = 'results/north'\n",
    "        self.out_dir = 'results/evaluation'\n",
    "        self.label_offset = 1\n",
    "        self.num_workers = 16\n",
    "        self.batch_size = 32\n",
    "        self.gpu_devices = '0, 1, 2, 3'\n",
    "\n",
    "\n",
    "# Initialize dummy args\n",
    "args = eval_args_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 20, \"PREDICTION CONFIG\", \"=\" * 20)\n",
    "for arg in vars(args):\n",
    "    print('{0:20}  {1}'.format(arg, getattr(args, arg)))\n",
    "print()\n",
    "\n",
    "# Load config of training\n",
    "train_args = pkl.load(open(args.args_path, \"rb\"))\n",
    "print(\"=\" * 20, \"TRAIN CONFIG\", \"=\" * 20)\n",
    "for arg in vars(train_args):\n",
    "    print('{0:20}  {1}'.format(arg, getattr(train_args, arg)))\n",
    "print()\n",
    "\n",
    "# Set flags for GPU processing if available\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Create output dir\n",
    "os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "# synchronize transform for validate dataset\n",
    "val_transform = Compose([\n",
    "    SyncToTensor()\n",
    "])\n",
    "\n",
    "# Image transform\n",
    "# Load mean and sd for normalization\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"means.pkl\"), \"rb\") as input_file:\n",
    "    mean = tuple(pkl.load(input_file))\n",
    "\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"stds.pkl\"), \"rb\") as input_file:\n",
    "    std = tuple(pkl.load(input_file))\n",
    "img_transform = ImgNorm(mean, std)\n",
    "\n",
    "# Get validate dataset\n",
    "validate_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                            usage='validate',\n",
    "                            label_offset=args.label_offset,\n",
    "                            sync_transform=val_transform,\n",
    "                            img_transform=img_transform,\n",
    "                            label_transform=None)\n",
    "# Put into DataLoader\n",
    "validate_loader = DataLoader(dataset=validate_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             num_workers=args.num_workers,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False)\n",
    "\n",
    "# set up network\n",
    "if train_args.model == \"deeplab\":\n",
    "    model = DeepLab(num_classes=train_args.n_classes,\n",
    "                    backbone='resnet',\n",
    "                    pretrained_backbone=False,\n",
    "                    output_stride=train_args.out_stride,\n",
    "                    sync_bn=False,\n",
    "                    freeze_bn=False,\n",
    "                    n_in=train_args.n_channels)\n",
    "else:\n",
    "    model = UNet(n_classes=train_args.n_classes,\n",
    "                 n_channels=train_args.n_channels)\n",
    "\n",
    "# Get devices\n",
    "if args.gpu_devices:\n",
    "    args.gpu_devices = [int(each) for each in args.gpu_devices.split(',')]\n",
    "\n",
    "if args.use_gpu:\n",
    "    if args.gpu_devices:\n",
    "        torch.cuda.set_device(args.gpu_devices[0])\n",
    "        model = torch.nn.DataParallel(model, device_ids=args.gpu_devices)\n",
    "    model = model.cuda()\n",
    "\n",
    "# Restore network weights\n",
    "state = torch.load(args.checkpoint_file)\n",
    "model.load_state_dict(state[\"model_state_dict\"])\n",
    "model.eval()\n",
    "print(\"Loaded checkpoint\")\n",
    "\n",
    "# predict samples\n",
    "# define metrics\n",
    "prec_score_ = Precision_score()\n",
    "recal_score_ = Recall_score()\n",
    "f1_score_ = F1_score()\n",
    "f2_score_ = F2_score()\n",
    "hamming_loss_ = Hamming_loss()\n",
    "types = validate_dataset.lc_types\n",
    "classification_report_ = cls_report(types)\n",
    "\n",
    "# Prediction\n",
    "y_true = []\n",
    "predicted_probs = []\n",
    "conf_mat = ConfMatrix(validate_loader.dataset.n_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (image, labels) in enumerate(tqdm(validate_loader, desc=\"Evaluate\")):\n",
    "        # Move data to gpu if model is on gpu\n",
    "        if args.use_gpu:\n",
    "            image = image.to(torch.device(\"cuda\"))\n",
    "        print(image.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(image)\n",
    "\n",
    "        # Update confusion matrix\n",
    "        conf_mat.add_batch(labels, logits.max(1)[1])\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        sm = torch.nn.Softmax(dim=1)\n",
    "        probs = sm(logits).cpu().numpy()\n",
    "\n",
    "        labels = labels.cpu().numpy()  # keep true & pred label at same loc.\n",
    "        predicted_probs += list(probs)\n",
    "        y_true += list(labels)\n",
    "\n",
    "predicted_probs = np.asarray(predicted_probs)\n",
    "\n",
    "# Convert predicted probabilities into one-hot labels\n",
    "y_predicted = np.argmax(predicted_probs, axis=1).flatten()\n",
    "y_true = np.asarray(y_true).flatten()\n",
    "\n",
    "# Evaluation with metrics\n",
    "f1 = f1_score_(y_predicted, y_true)\n",
    "f2 = f2_score_(y_predicted, y_true)\n",
    "prec = prec_score_(y_predicted, y_true)\n",
    "rec = recal_score_(y_predicted, y_true)\n",
    "hm_loss = hamming_loss_(y_predicted, y_true)\n",
    "report = classification_report_(y_predicted, y_true)\n",
    "aa = conf_mat.get_aa()\n",
    "\n",
    "info = {\"weightedPrec\": prec,\n",
    "        \"weightedRec\": rec,\n",
    "        \"weightedF1\": f1,\n",
    "        \"weightedF2\": f2,\n",
    "        \"HammingLoss\": hm_loss,\n",
    "        \"clsReport\": report,\n",
    "        \"conf_mat\": conf_mat.norm_on_lines(),\n",
    "        \"AverageAcc\": aa}\n",
    "\n",
    "print(\"Save out metrics\")\n",
    "pkl.dump(info,open(os.path.join(\n",
    "                 args.out_dir,\"{}_evaluation.pkl\"\n",
    "                     .format(train_args.exp_name)), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do one prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from augmentation import *\n",
    "from dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "from models.deeplab import DeepLab\n",
    "from models.unet import UNet\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_args_dummy:\n",
    "    def __init__(self):\n",
    "        self.args_path = 'results/dl/unet_norm_adabound_compose_lr/checkpoints/args.pkl'\n",
    "        self.checkpoint_file = 'results/dl/unet_norm_adabound_compose_lr/checkpoints/final.pth'\n",
    "        self.stats_dir = 'results/north/norm_stats'\n",
    "        self.data_dir = 'results/north'\n",
    "        self.out_dir = 'results/prediction'\n",
    "        self.label_offset = 1\n",
    "        self.num_workers = 16\n",
    "        self.batch_size = 32\n",
    "        self.gpu_devices = '0, 1, 2, 3'\n",
    "\n",
    "\n",
    "# Initialize dummy args\n",
    "args = pred_args_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 20, \"PREDICTION CONFIG\", \"=\" * 20)\n",
    "for arg in vars(args):\n",
    "    print('{0:20}  {1}'.format(arg, getattr(args, arg)))\n",
    "print()\n",
    "\n",
    "# Load config of training\n",
    "train_args = pkl.load(open(args.args_path, \"rb\"))\n",
    "print(\"=\" * 20, \"TRAIN CONFIG\", \"=\" * 20)\n",
    "for arg in vars(train_args):\n",
    "    print('{0:20}  {1}'.format(arg, getattr(train_args, arg)))\n",
    "print()\n",
    "\n",
    "# Create output dir\n",
    "os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "# Set flags for GPU processing if available\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# set up network\n",
    "if train_args.model == \"deeplab\":\n",
    "    model = DeepLab(num_classes=train_args.n_classes,\n",
    "                    backbone='resnet',\n",
    "                    pretrained_backbone=False,\n",
    "                    output_stride=train_args.out_stride,\n",
    "                    sync_bn=False,\n",
    "                    freeze_bn=False,\n",
    "                    n_in=train_args.n_channels)\n",
    "else:\n",
    "    model = UNet(n_classes=train_args.n_classes,\n",
    "                 n_channels=train_args.n_channels)\n",
    "# Get devices\n",
    "if args.gpu_devices:\n",
    "    args.gpu_devices = [int(each) for each in args.gpu_devices.split(',')]\n",
    "\n",
    "if args.use_gpu:\n",
    "    if args.gpu_devices:\n",
    "        torch.cuda.set_device(args.gpu_devices[0])\n",
    "        model = torch.nn.DataParallel(model, device_ids=args.gpu_devices)\n",
    "    model = model.cuda()\n",
    "\n",
    "# Restore network weights\n",
    "state = torch.load(args.checkpoint_file)\n",
    "model.load_state_dict(state[\"model_state_dict\"])\n",
    "model.eval()\n",
    "print(\"Loaded checkpoint from {}\".format(args.checkpoint_file))\n",
    "\n",
    "# Predict\n",
    "# Image transform\n",
    "# Load mean and sd for normalization\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"means.pkl\"), \"rb\") as input_file:\n",
    "    mean = tuple(pkl.load(input_file))\n",
    "\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"stds.pkl\"), \"rb\") as input_file:\n",
    "    std = tuple(pkl.load(input_file))\n",
    "pred_transform = ComposeImg([\n",
    "    ImgToTensor(),\n",
    "    ImgNorm(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(os.path.join(args.data_dir, 'dl_catalog_predict.csv'))\n",
    "tile_id = '1224-997'\n",
    "# Get validate dataset\n",
    "predict_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                           usage='predict',\n",
    "                           sync_transform=None,\n",
    "                           img_transform=pred_transform,\n",
    "                           label_transform=None,\n",
    "                           tile_id=tile_id)\n",
    "# Put into DataLoader\n",
    "predict_loader = DataLoader(dataset=predict_dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            num_workers=args.num_workers,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False)\n",
    "\n",
    "# Set and crate paths\n",
    "score_path = os.path.join(args.out_dir, 'score')\n",
    "class_path = os.path.join(args.out_dir, 'class')\n",
    "if not os.path.isdir(score_path):\n",
    "    os.mkdir(score_path)\n",
    "if not os.path.isdir(class_path):\n",
    "    os.mkdir(class_path)\n",
    "\n",
    "# File names\n",
    "name_score = os.path.join(score_path, 'score_{}'.format(tile_id))\n",
    "name_class = os.path.join(class_path, 'class_{}.tif'.format(tile_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy tile\n",
    "meta = predict_dataset.meta\n",
    "n_class = predict_dataset.n_classes\n",
    "canvas = np.zeros((1, meta['height'], meta['width']),\n",
    "                  dtype=meta['dtype'])\n",
    "canvas_score_ls = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (img, index_full) in enumerate(tqdm(predict_loader, desc=\"Predict\")):\n",
    "        # GPU setting\n",
    "        if args.use_gpu:\n",
    "            img = img.cuda()\n",
    "\n",
    "        out = F.softmax(model(img), 1)\n",
    "        batch, n_class, width, height = out.size()\n",
    "        score_width = width\n",
    "        score_height = height\n",
    "\n",
    "        # for each batch\n",
    "        for i in range(batch):\n",
    "            index = (index_full[0][i], index_full[1][i])\n",
    "            out_predict = out.max(dim=1)[1][:, :, :].cpu().numpy()[i, :, :]\n",
    "            out_predict = np.expand_dims(out_predict, axis=0)\n",
    "            out_predict = out_predict.astype(np.int8)\n",
    "            canvas[:, index[0]: index[0] + score_width, index[1]: index[1] + score_height] = out_predict\n",
    "\n",
    "            # scores for each non-background class\n",
    "            for n in range(n_class):\n",
    "                out_score = out[:, n, :, :].data[i][:, :].cpu().numpy() * 100\n",
    "                out_score = np.expand_dims(out_score, axis=0).astype(np.int8)\n",
    "                try:\n",
    "                    canvas_score_ls[n][:, index[0]: index[0] + score_width, index[1]: index[1] + score_height] = \\\n",
    "                        out_score\n",
    "                except:\n",
    "                    canvas_score_single = np.zeros((1, meta['height'], meta['width']), dtype=meta['dtype'])\n",
    "                    canvas_score_single[:, index[0]: index[0] + score_width, index[1]: index[1] + score_height] = \\\n",
    "                        out_score\n",
    "                    canvas_score_ls.append(canvas_score_single)\n",
    "\n",
    "# Save out\n",
    "with rasterio.open(name_class, 'w', **meta) as dst:\n",
    "    dst.write(canvas)\n",
    "\n",
    "for n in range(n_class):\n",
    "    with rasterio.open('{}_class{}.tif'.format(name_score, n), 'w', **meta) as dst:\n",
    "        dst.write(canvas_score_ls[n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_latest_p36]",
   "language": "python",
   "name": "conda-env-pytorch_latest_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
