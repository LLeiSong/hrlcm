{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/hrlcm/hrlcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from augmentation import *\n",
    "from dataset import *\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle as pkl\n",
    "from models.deeplab import DeepLab\n",
    "from models.unet import UNet\n",
    "from train import Trainer\n",
    "from loss import BalancedCrossEntropyLoss\n",
    "from sync_batchnorm import convert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/ubuntu/hrlcm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummy inline arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy args for testing\n",
    "class args_dummy:\n",
    "    def __init__(self):\n",
    "        self.exp_name = 'unet_test'\n",
    "        self.data_dir = 'results/north'\n",
    "        self.out_dir = 'results/dl'\n",
    "        self.lowest_score = 10\n",
    "        self.noise_ratio = 0\n",
    "        self.trans_prob = 0.5\n",
    "        self.label_offset = 1\n",
    "        self.rg_rotate = '-90, 90'\n",
    "        self.model = 'unet'\n",
    "        self.train_mode = 'single'\n",
    "        self.out_stride = 8\n",
    "        self.gpu_devices = '0, 1, 2, 3'\n",
    "        self.sync_norm = False\n",
    "        self.lr = 0.001\n",
    "        self.decay = 1e-5\n",
    "        self.save_freq = 10\n",
    "        self.log_feq = 10\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 100\n",
    "        self.optimizer_name = 'Adam'\n",
    "        self.resume = None\n",
    "        self.checkpoint_dir = os.path.join(self.out_dir, self.exp_name, 'checkpoints')\n",
    "        self.logs_dir = os.path.join(self.out_dir, self.exp_name, 'logs')\n",
    "\n",
    "\n",
    "# Initialize dummy args\n",
    "args = args_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory for saving files\n",
    "if args.exp_name:\n",
    "    args.checkpoint_dir = os.path.join(args.out_dir, args.exp_name, 'checkpoints')\n",
    "    args.logs_dir = os.path.join(args.out_dir, args.exp_name, 'logs')\n",
    "else:\n",
    "    args.checkpoint_dir = os.path.join(args.out_dir, args.model, 'checkpoints')\n",
    "    args.logs_dir = os.path.join(args.out_dir, args.model, 'logs')\n",
    "\n",
    "# Create dirs if necessary\n",
    "if not os.path.isdir(args.checkpoint_dir):\n",
    "    os.makedirs(args.checkpoint_dir)\n",
    "if not os.path.isdir(args.logs_dir):\n",
    "    os.makedirs(args.logs_dir)\n",
    "\n",
    "# Dir for mean and sd pickles\n",
    "args.stats_dir = os.path.join(args.data_dir, 'norm_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flags for GPU processing if available\n",
    "if torch.cuda.is_available():\n",
    "    args.use_gpu = True\n",
    "else:\n",
    "    args.use_gpu = False\n",
    "\n",
    "# Load dataset\n",
    "# Define rotate degrees\n",
    "args.rg_rotate = tuple(float(each) for each in args.rg_rotate.split(','))\n",
    "\n",
    "# synchronize transform for train dataset\n",
    "sync_transform = Compose([\n",
    "    RandomScale(prob=args.trans_prob),\n",
    "    RandomFlip(prob=args.trans_prob),\n",
    "    RandomCenterRotate(degree=args.rg_rotate,\n",
    "                       prob=args.trans_prob),\n",
    "    SyncToTensor()\n",
    "])\n",
    "\n",
    "# synchronize transform for validate dataset\n",
    "val_transform = Compose([\n",
    "    SyncToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train dataset\n",
    "train_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                         usage='train',\n",
    "                         lowest_score=args.lowest_score,\n",
    "                         noise_ratio=args.noise_ratio,\n",
    "                         label_offset=args.label_offset,\n",
    "                         sync_transform=sync_transform,\n",
    "                         img_transform=None,\n",
    "                         label_transform=None)\n",
    "# Put into DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "# Get validate dataset\n",
    "validate_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                            usage='validate',\n",
    "                            label_offset=args.label_offset,\n",
    "                            sync_transform=val_transform,\n",
    "                            img_transform=None,\n",
    "                            label_transform=None)\n",
    "# Put into DataLoader\n",
    "validate_loader = DataLoader(dataset=validate_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.gpu_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up network\n",
    "args.n_classes = train_dataset.n_classes\n",
    "args.n_channels = train_dataset.n_channels\n",
    "if args.model == \"deeplab\":\n",
    "    model = DeepLab(num_classes=args.n_classes,\n",
    "                    backbone='resnet',\n",
    "                    pretrained_backbone=False,\n",
    "                    output_stride=args.out_stride,\n",
    "                    sync_bn=False,\n",
    "                    freeze_bn=False,\n",
    "                    n_in=args.n_channels)\n",
    "else:\n",
    "    model = UNet(n_classes=args.n_classes,\n",
    "                 n_channels=args.n_channels)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "    # Get devices\n",
    "if args.gpu_devices:\n",
    "    args.gpu_devices = [int(each) for each in args.gpu_devices.split(',')]\n",
    "\n",
    "# Set model\n",
    "if args.use_gpu:\n",
    "    if args.gpu_devices:\n",
    "        torch.cuda.set_device(args.gpu_devices[0])\n",
    "        model = torch.nn.DataParallel(model, device_ids=args.gpu_devices)\n",
    "        if args.sync_norm:\n",
    "            model = convert_model(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = BalancedCrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "if args.optimizer_name == 'Adadelta':\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(),\n",
    "                                     lr=args.lr,\n",
    "                                     weight_decay=args.decay)\n",
    "elif args.optimizer_name == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr)\n",
    "else:\n",
    "    print('Not supported optimizer, use Adam instead.')\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr)\n",
    "\n",
    "# Set up tensorboard logging\n",
    "writer = SummaryWriter(log_dir=args.logs_dir)\n",
    "\n",
    "# Save config\n",
    "pkl.dump(args, open(os.path.join(args.checkpoint_dir, \"args.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f4020924ab464d97b5f8e08445474d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[Epoch]', style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b425aab7d6b494fbf064f4f473fd0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[Train]', max=114.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_latest_p36]",
   "language": "python",
   "name": "conda-env-pytorch_latest_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
