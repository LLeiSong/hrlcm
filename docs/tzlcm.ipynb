{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/hrlcm/hrlcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from augmentation import *\n",
    "from dataset import *\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle as pkl\n",
    "from models.deeplab import DeepLab\n",
    "from models.unet import UNet\n",
    "from train import Trainer\n",
    "from loss import BalancedCrossEntropyLoss\n",
    "import torch_optimizer as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/ubuntu/hrlcm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummy inline arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy args for testing\n",
    "class args_dummy:\n",
    "    def __init__(self):\n",
    "        self.exp_name = 'unet_norm_adabound_compose_lr'\n",
    "        self.data_dir = 'results/north'\n",
    "        self.out_dir = 'results/dl'\n",
    "        self.lowest_score = 10\n",
    "        self.noise_ratio = 0\n",
    "        self.trans_prob = 0.5\n",
    "        self.label_offset = 1\n",
    "        self.rg_rotate = '-90, 90'\n",
    "        self.model = 'unet'\n",
    "        self.train_mode = 'single'\n",
    "        self.out_stride = 8\n",
    "        self.gpu_devices = '0, 1, 2, 3'\n",
    "        self.sync_norm = False\n",
    "        self.max_lr = 0.001\n",
    "        self.base_lr = 0.001\n",
    "        self.clr_gamma = 0.9999\n",
    "        self.save_freq = 10\n",
    "        self.log_feq = 10\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 200\n",
    "        self.optimizer_name = 'AdamP'\n",
    "        self.resume = None\n",
    "        self.checkpoint_dir = os.path.join(self.out_dir, self.exp_name, 'checkpoints')\n",
    "        self.logs_dir = os.path.join(self.out_dir, self.exp_name, 'logs')\n",
    "\n",
    "\n",
    "# Initialize dummy args\n",
    "args = args_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory for saving files\n",
    "if args.exp_name:\n",
    "    args.checkpoint_dir = os.path.join(args.out_dir, args.exp_name, 'checkpoints')\n",
    "    args.logs_dir = os.path.join(args.out_dir, args.exp_name, 'logs')\n",
    "else:\n",
    "    args.checkpoint_dir = os.path.join(args.out_dir, args.model, 'checkpoints')\n",
    "    args.logs_dir = os.path.join(args.out_dir, args.model, 'logs')\n",
    "\n",
    "# Create dirs if necessary\n",
    "if not os.path.isdir(args.checkpoint_dir):\n",
    "    os.makedirs(args.checkpoint_dir)\n",
    "if not os.path.isdir(args.logs_dir):\n",
    "    os.makedirs(args.logs_dir)\n",
    "\n",
    "# Dir for mean and sd pickles\n",
    "args.stats_dir = os.path.join(args.data_dir, 'norm_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flags for GPU processing if available\n",
    "if torch.cuda.is_available():\n",
    "    args.use_gpu = True\n",
    "else:\n",
    "    args.use_gpu = False\n",
    "\n",
    "# Load dataset\n",
    "# Define rotate degrees\n",
    "args.rg_rotate = tuple(float(each) for each in args.rg_rotate.split(','))\n",
    "\n",
    "# synchronize transform for train dataset\n",
    "sync_transform = Compose([\n",
    "#     RandomScale(prob=args.trans_prob),\n",
    "    RandomFlip(prob=args.trans_prob),\n",
    "#     RandomCenterRotate(degree=args.rg_rotate,\n",
    "#                        prob=args.trans_prob),\n",
    "    SyncToTensor()\n",
    "])\n",
    "\n",
    "# synchronize transform for validate dataset\n",
    "val_transform = Compose([\n",
    "    SyncToTensor()\n",
    "])\n",
    "\n",
    "# Image transform\n",
    "# Load mean and sd for normalization\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"means.pkl\"), \"rb\") as input_file:\n",
    "    mean = tuple(pkl.load(input_file))\n",
    "\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"stds.pkl\"), \"rb\") as input_file:\n",
    "    std = tuple(pkl.load(input_file))\n",
    "img_transform = ImgNorm(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train dataset\n",
    "train_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                         usage='train',\n",
    "                         lowest_score=args.lowest_score,\n",
    "                         noise_ratio=args.noise_ratio,\n",
    "                         label_offset=args.label_offset,\n",
    "                         sync_transform=sync_transform,\n",
    "                         img_transform=img_transform,\n",
    "                         label_transform=None)\n",
    "# Put into DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=args.batch_size,\n",
    "                          num_workers=32,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "# Get validate dataset\n",
    "validate_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                            usage='validate',\n",
    "                            label_offset=args.label_offset,\n",
    "                            sync_transform=val_transform,\n",
    "                            img_transform=img_transform,\n",
    "                            label_transform=None)\n",
    "# Put into DataLoader\n",
    "validate_loader = DataLoader(dataset=validate_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             num_workers=32,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up network\n",
    "args.n_classes = train_dataset.n_classes\n",
    "args.n_channels = train_dataset.n_channels\n",
    "if args.model == \"deeplab\":\n",
    "    model = DeepLab(num_classes=args.n_classes,\n",
    "                    backbone='resnet',\n",
    "                    pretrained_backbone=False,\n",
    "                    output_stride=args.out_stride,\n",
    "                    sync_bn=False,\n",
    "                    freeze_bn=False,\n",
    "                    n_in=args.n_channels)\n",
    "else:\n",
    "    model = UNet(n_classes=args.n_classes,\n",
    "                 n_channels=args.n_channels)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "    # Get devices\n",
    "if args.gpu_devices:\n",
    "    args.gpu_devices = [int(each) for each in args.gpu_devices.split(',')]\n",
    "\n",
    "# Set model\n",
    "if args.use_gpu:\n",
    "    if args.gpu_devices:\n",
    "        torch.cuda.set_device(args.gpu_devices[0])\n",
    "        model = torch.nn.DataParallel(model, device_ids=args.gpu_devices)\n",
    "        if args.sync_norm:\n",
    "            model = convert_model(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = BalancedCrossEntropyLoss()\n",
    "\n",
    "# Set up tensorboard logging\n",
    "writer = SummaryWriter(log_dir=args.logs_dir)\n",
    "\n",
    "# Save config\n",
    "pkl.dump(args, open(os.path.join(args.checkpoint_dir, \"args.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "# if args.optimizer_name == 'Adadelta':\n",
    "#     optimizer = torch.optim.Adadelta(model.parameters(),\n",
    "#                                      lr=args.max_lr)\n",
    "# elif args.optimizer_name == 'Adam':\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# elif args.optimizer_name == 'AdamW':\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# else:\n",
    "#     print('Not supported optimizer, use Adam instead.')\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "optimizer = optim.AdaBound(model.parameters(), lr=0.001, final_lr=0.01, amsbound=True)\n",
    "# optimizer = optim.AdamP(model.parameters(), nesterov=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With constant learning rate\n",
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.set_description(\"[Epoch] lr: {:.4f}\".format(\n",
    "                round(optimizer.param_groups[0][\"lr\"], 4)))\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With learning rate scheduler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "# if args.optimizer_name == 'Adadelta':\n",
    "#     optimizer = torch.optim.Adadelta(model.parameters(),\n",
    "#                                      lr=args.max_lr)\n",
    "# elif args.optimizer_name == 'Adam':\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# elif args.optimizer_name == 'AdamW':\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# else:\n",
    "#     print('Not supported optimizer, use Adam instead.')\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                                  lr=args.max_lr,\n",
    "#                                  amsgrad=True)\n",
    "# optimizer = optim.AdaBound(model.parameters(), lr=0.001, final_lr=0.01, amsbound=True)\n",
    "optimizer = optim.AdamP(model.parameters(), nesterov=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.set_description(\"[Epoch] lr: {:.4f}\".format(\n",
    "                round(optimizer.param_groups[0][\"lr\"], 4)))\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composed scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate curve\n",
    "import matplotlib.pyplot as plt\n",
    "lr_scheduler_1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.001)\n",
    "lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.005, \n",
    "                                                   step_size_up=1, step_size_down=3,\n",
    "                                                   gamma=0.988, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "\n",
    "lrs = []\n",
    "\n",
    "for i in range(200):\n",
    "    optimizer.step()\n",
    "    if i <= lr_scheduler_1.T_max:\n",
    "        lr_scheduler_1.step()\n",
    "    else:\n",
    "        lr_scheduler_2.step()\n",
    "    lrs.append(\n",
    "        optimizer.param_groups[0][\"lr\"]\n",
    "    )\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.001)\n",
    "lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.05, \n",
    "                                                   step_size_up=1, step_size_down=3,\n",
    "                                                   gamma=0.96, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "    \n",
    "    # Update learning rate\n",
    "    if epoch <= lr_scheduler_1.T_max:\n",
    "        lr_scheduler_1.step()\n",
    "    else:\n",
    "        lr_scheduler_2.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.set_description(\"[Epoch] lr: {:.4f}\".format(\n",
    "                round(optimizer.param_groups[0][\"lr\"], 4)))\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composed optimizer and scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.AdaBound(model.parameters(), lr=0.001, final_lr=0.01, amsbound=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lrs = []\n",
    "\n",
    "for i in range(200):\n",
    "    optimizer.step()\n",
    "    if i <= 30:\n",
    "        if i == 30:\n",
    "            lr_scheduler_1 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0008, max_lr=0.0012, \n",
    "                                                   step_size_up=1, step_size_down=3,\n",
    "                                                   gamma=0.97, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "    elif i <= 120:\n",
    "        lr_scheduler_1.step()\n",
    "        if i == 120:\n",
    "            lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0004, max_lr=0.0006, \n",
    "                                                   step_size_up=1, step_size_down=5,\n",
    "                                                   gamma=0.94, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "    else:\n",
    "        lr_scheduler_2.step()\n",
    "    lrs.append(\n",
    "        optimizer.param_groups[0][\"lr\"]\n",
    "    )\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.AdaBound(model.parameters(), lr=0.001, final_lr=0.01, amsbound=True)\n",
    "\n",
    "step = 0\n",
    "trainer = Trainer(args)\n",
    "pbar = tqdm(total=args.epochs, desc=\"[Epoch]\")\n",
    "for epoch in range(args.epochs):\n",
    "    # Run training for one epoch\n",
    "    model, step = trainer.train(model, train_loader, loss_fn,\n",
    "                                optimizer, writer, step=step)\n",
    "    # Run validation\n",
    "    trainer.validate(model, validate_loader, step, loss_fn, writer)\n",
    "    \n",
    "    # Update learning rate\n",
    "    if epoch <= 30:\n",
    "        if epoch == 30:\n",
    "            lr_scheduler_1 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0008, max_lr=0.0012, \n",
    "                                                   step_size_up=1, step_size_down=3,\n",
    "                                                   gamma=0.97, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "    elif epoch > 30 and epoch <= 120:\n",
    "        lr_scheduler_1.step()\n",
    "        if epoch == 120:\n",
    "            lr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0004, max_lr=0.0006, \n",
    "                                                   step_size_up=1, step_size_down=5,\n",
    "                                                   gamma=0.94, cycle_momentum=False,\n",
    "                                                   mode='exp_range')\n",
    "    else:\n",
    "        lr_scheduler_2.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % args.save_freq == 0:\n",
    "        trainer.export_model(model, optimizer=optimizer, step=step)\n",
    "\n",
    "    # Update pbar\n",
    "    pbar.set_description(\"[Epoch] lr: {:.4f}\".format(\n",
    "                round(optimizer.param_groups[0][\"lr\"], 4)))\n",
    "    pbar.update()\n",
    "\n",
    "# Export final set of weights\n",
    "trainer.export_model(model, optimizer, name=\"final\")\n",
    "\n",
    "# Close pbar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and load packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from augmentation import *\n",
    "from dataset import *\n",
    "from metrics import ConfMatrix\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle as pkl\n",
    "from models.deeplab import DeepLab\n",
    "from models.unet import UNet\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, \\\n",
    "    fbeta_score, classification_report, hamming_loss\n",
    "\n",
    "class Precision_score(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        weighted_prec = precision_score(true_labels, predict_labels, average='weighted')\n",
    "\n",
    "        return weighted_prec\n",
    "\n",
    "\n",
    "class Recall_score(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        weighted_rec = recall_score(true_labels, predict_labels, average='weighted')\n",
    "\n",
    "        return weighted_rec\n",
    "\n",
    "\n",
    "class F1_score(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        weighted_f1 = f1_score(true_labels, predict_labels, average=\"weighted\")\n",
    "\n",
    "        return weighted_f1\n",
    "\n",
    "\n",
    "class F2_score(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        weighted_f2 = fbeta_score(true_labels, predict_labels, beta=2, average=\"weighted\")\n",
    "\n",
    "        return weighted_f2\n",
    "\n",
    "\n",
    "class Hamming_loss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(predict_labels, true_labels):\n",
    "        return hamming_loss(true_labels, predict_labels)\n",
    "\n",
    "\n",
    "class cls_report(nn.Module):\n",
    "    def __init__(self, target_names):\n",
    "        super().__init__()\n",
    "        self.target_names = target_names\n",
    "\n",
    "    def forward(self, predict_labels, true_labels):\n",
    "        report = classification_report(true_labels, predict_labels,\n",
    "                                       target_names=self.target_names,\n",
    "                                       output_dict=True)\n",
    "\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eval_args_dummy:\n",
    "    def __init__(self):\n",
    "        self.args_path = 'results/dl/unet_norm_adabound_compose_lr/checkpoints/args.pkl'\n",
    "        self.checkpoint_file = 'results/dl/unet_norm_adabound_compose_lr/checkpoints/final.pth'\n",
    "        self.stats_dir = 'results/north/norm_stats'\n",
    "        self.data_dir = 'results/north'\n",
    "        self.out_dir = 'results/evaluation'\n",
    "        self.label_offset = 1\n",
    "        self.num_workers = 16\n",
    "        self.batch_size = 32\n",
    "        self.gpu_devices = '0, 1, 2, 3'\n",
    "\n",
    "\n",
    "# Initialize dummy args\n",
    "args = eval_args_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== PREDICTION CONFIG ====================\n",
      "args_path             results/dl/unet_norm_adabound_compose_lr/checkpoints/args.pkl\n",
      "checkpoint_file       results/dl/unet_norm_adabound_compose_lr/checkpoints/final.pth\n",
      "stats_dir             results/north/norm_stats\n",
      "data_dir              results/north\n",
      "out_dir               results/evaluation\n",
      "label_offset          1\n",
      "num_workers           16\n",
      "batch_size            32\n",
      "gpu_devices           0, 1, 2, 3\n",
      "\n",
      "==================== TRAIN CONFIG ====================\n",
      "exp_name              unet_norm_adabound_compose_lr\n",
      "data_dir              results/north\n",
      "out_dir               results/dl\n",
      "lowest_score          10\n",
      "noise_ratio           0\n",
      "trans_prob            0.5\n",
      "label_offset          1\n",
      "rg_rotate             (-90.0, 90.0)\n",
      "model                 unet\n",
      "train_mode            single\n",
      "out_stride            8\n",
      "gpu_devices           [0, 1, 2, 3]\n",
      "sync_norm             False\n",
      "max_lr                0.001\n",
      "base_lr               0.001\n",
      "clr_gamma             0.9999\n",
      "save_freq             10\n",
      "log_feq               10\n",
      "batch_size            32\n",
      "epochs                200\n",
      "optimizer_name        AdamP\n",
      "resume                None\n",
      "checkpoint_dir        results/dl/unet_norm_adabound_compose_lr/checkpoints\n",
      "logs_dir              results/dl/unet_norm_adabound_compose_lr/logs\n",
      "stats_dir             results/north/norm_stats\n",
      "use_gpu               True\n",
      "n_classes             7\n",
      "n_channels            14\n",
      "\n",
      "Loaded checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ecc271752a48788799d5b6c0c9d532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluate', max=30.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save out metrics\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 20, \"PREDICTION CONFIG\", \"=\" * 20)\n",
    "for arg in vars(args):\n",
    "    print('{0:20}  {1}'.format(arg, getattr(args, arg)))\n",
    "print()\n",
    "\n",
    "# Load config of training\n",
    "train_args = pkl.load(open(args.args_path, \"rb\"))\n",
    "print(\"=\" * 20, \"TRAIN CONFIG\", \"=\" * 20)\n",
    "for arg in vars(train_args):\n",
    "    print('{0:20}  {1}'.format(arg, getattr(train_args, arg)))\n",
    "print()\n",
    "\n",
    "# Set flags for GPU processing if available\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Create output dir\n",
    "os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "# synchronize transform for validate dataset\n",
    "val_transform = Compose([\n",
    "    SyncToTensor()\n",
    "])\n",
    "\n",
    "# Image transform\n",
    "# Load mean and sd for normalization\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"means.pkl\"), \"rb\") as input_file:\n",
    "    mean = tuple(pkl.load(input_file))\n",
    "\n",
    "with open(os.path.join(args.stats_dir,\n",
    "                       \"stds.pkl\"), \"rb\") as input_file:\n",
    "    std = tuple(pkl.load(input_file))\n",
    "img_transform = ImgNorm(mean, std)\n",
    "\n",
    "# Get validate dataset\n",
    "validate_dataset = NFSEN1LC(data_dir=args.data_dir,\n",
    "                            usage='validate',\n",
    "                            label_offset=args.label_offset,\n",
    "                            sync_transform=val_transform,\n",
    "                            img_transform=img_transform,\n",
    "                            label_transform=None)\n",
    "# Put into DataLoader\n",
    "validate_loader = DataLoader(dataset=validate_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             num_workers=args.num_workers,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False)\n",
    "\n",
    "# set up network\n",
    "if train_args.model == \"deeplab\":\n",
    "    model = DeepLab(num_classes=train_args.n_classes,\n",
    "                    backbone='resnet',\n",
    "                    pretrained_backbone=False,\n",
    "                    output_stride=train_args.out_stride,\n",
    "                    sync_bn=False,\n",
    "                    freeze_bn=False,\n",
    "                    n_in=train_args.n_channels)\n",
    "else:\n",
    "    model = UNet(n_classes=train_args.n_classes,\n",
    "                 n_channels=train_args.n_channels)\n",
    "\n",
    "# Get devices\n",
    "if args.gpu_devices:\n",
    "    args.gpu_devices = [int(each) for each in args.gpu_devices.split(',')]\n",
    "\n",
    "if args.use_gpu:\n",
    "    if args.gpu_devices:\n",
    "        torch.cuda.set_device(args.gpu_devices[0])\n",
    "        model = torch.nn.DataParallel(model, device_ids=args.gpu_devices)\n",
    "    model = model.cuda()\n",
    "\n",
    "# Restore network weights\n",
    "state = torch.load(args.checkpoint_file)\n",
    "model.load_state_dict(state[\"model_state_dict\"])\n",
    "model.eval()\n",
    "print(\"Loaded checkpoint\")\n",
    "\n",
    "# predict samples\n",
    "# define metrics\n",
    "prec_score_ = Precision_score()\n",
    "recal_score_ = Recall_score()\n",
    "f1_score_ = F1_score()\n",
    "f2_score_ = F2_score()\n",
    "hamming_loss_ = Hamming_loss()\n",
    "types = validate_dataset.lc_types\n",
    "classification_report_ = cls_report(types)\n",
    "\n",
    "# Prediction\n",
    "y_true = []\n",
    "predicted_probs = []\n",
    "conf_mat = ConfMatrix(validate_loader.dataset.n_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (image, labels) in enumerate(tqdm(validate_loader, desc=\"Evaluate\")):\n",
    "        # Move data to gpu if model is on gpu\n",
    "        if args.use_gpu:\n",
    "            image = image.to(torch.device(\"cuda\"))\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(image)\n",
    "\n",
    "        # Update confusion matrix\n",
    "        conf_mat.add_batch(labels, logits.max(1)[1])\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        sm = torch.nn.Softmax(dim=1)\n",
    "        probs = sm(logits).cpu().numpy()\n",
    "\n",
    "        labels = labels.cpu().numpy()  # keep true & pred label at same loc.\n",
    "        predicted_probs += list(probs)\n",
    "        y_true += list(labels)\n",
    "\n",
    "predicted_probs = np.asarray(predicted_probs)\n",
    "\n",
    "# Convert predicted probabilities into one-hot labels\n",
    "y_predicted = np.argmax(predicted_probs, axis=1).flatten()\n",
    "y_true = np.asarray(y_true).flatten()\n",
    "\n",
    "# Evaluation with metrics\n",
    "f1 = f1_score_(y_predicted, y_true)\n",
    "f2 = f2_score_(y_predicted, y_true)\n",
    "prec = prec_score_(y_predicted, y_true)\n",
    "rec = recal_score_(y_predicted, y_true)\n",
    "hm_loss = hamming_loss_(y_predicted, y_true)\n",
    "report = classification_report_(y_predicted, y_true)\n",
    "aa = conf_mat.get_aa()\n",
    "\n",
    "info = {\"weightedPrec\": prec,\n",
    "        \"weightedRec\": rec,\n",
    "        \"weightedF1\": f1,\n",
    "        \"weightedF2\": f2,\n",
    "        \"HammingLoss\": hm_loss,\n",
    "        \"clsReport\": report,\n",
    "        \"conf_mat\": conf_mat.norm_on_lines(),\n",
    "        \"AverageAcc\": aa}\n",
    "\n",
    "print(\"Save out metrics\")\n",
    "pkl.dump(info,open(os.path.join(\n",
    "                 args.out_dir,\"{}_evaluation.pkl\"\n",
    "                     .format(train_args.exp_name)), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weightedPrec': 0.9230187367229913,\n",
       " 'weightedRec': 0.8719084136833852,\n",
       " 'weightedF1': 0.8910451918369202,\n",
       " 'weightedF2': 0.8769002324638219,\n",
       " 'HammingLoss': 0.12809158631661477,\n",
       " 'clsReport': {1: {'precision': 0.9688965941477616,\n",
       "   'recall': 0.8606892690609775,\n",
       "   'f1-score': 0.9115930748942007,\n",
       "   'support': 117000348},\n",
       "  2: {'precision': 0.8706323559609628,\n",
       "   'recall': 0.9936787315410746,\n",
       "   'f1-score': 0.9280949525103989,\n",
       "   'support': 21240199},\n",
       "  3: {'precision': 0.9441417141427242,\n",
       "   'recall': 0.8445328546926495,\n",
       "   'f1-score': 0.8915637433124971,\n",
       "   'support': 32774481},\n",
       "  4: {'precision': 0.9327622564515463,\n",
       "   'recall': 0.8593451070239672,\n",
       "   'f1-score': 0.8945498438706945,\n",
       "   'support': 54873363},\n",
       "  5: {'precision': 0.9753786097388636,\n",
       "   'recall': 0.9748271637916003,\n",
       "   'f1-score': 0.9751028088009892,\n",
       "   'support': 11212205},\n",
       "  6: {'precision': 0.0929670550393574,\n",
       "   'recall': 0.9259246708600581,\n",
       "   'f1-score': 0.16896886617105328,\n",
       "   'support': 855791},\n",
       "  7: {'precision': 0.3888492401198662,\n",
       "   'recall': 0.7834556123669075,\n",
       "   'f1-score': 0.519738733300097,\n",
       "   'support': 10031837},\n",
       "  'accuracy': 0.8719084136833852,\n",
       "  'macro avg': {'precision': 0.7390896893715831,\n",
       "   'recall': 0.8917790584767479,\n",
       "   'f1-score': 0.7556588604085617,\n",
       "   'support': 247988224},\n",
       "  'weighted avg': {'precision': 0.9230187367229913,\n",
       "   'recall': 0.8719084136833852,\n",
       "   'f1-score': 0.8910451918369202,\n",
       "   'support': 247988224}},\n",
       " 'conf_mat': array([[8.60689252e-01, 4.82061814e-03, 8.30147958e-03, 2.24450187e-02,\n",
       "         4.40306383e-04, 3.39759930e-02, 6.93273323e-02],\n",
       "        [2.35026047e-04, 9.93678732e-01, 9.74567140e-06, 3.30557167e-03,\n",
       "         2.09037590e-05, 2.52078618e-03, 2.29235140e-04],\n",
       "        [3.58533824e-02, 5.20404885e-04, 8.44532855e-01, 1.71296992e-02,\n",
       "         4.84410417e-03, 1.16744488e-02, 8.54451059e-02],\n",
       "        [2.43211629e-02, 4.61645662e-02, 8.02110488e-03, 8.59345107e-01,\n",
       "         1.73563264e-04, 4.05964183e-02, 2.13780774e-02],\n",
       "        [9.23993095e-04, 7.73175303e-04, 1.63687696e-03, 1.56704234e-04,\n",
       "         9.74827164e-01, 7.60778099e-05, 2.16060088e-02],\n",
       "        [3.74320366e-02, 3.89697952e-03, 1.72472017e-03, 6.77034463e-03,\n",
       "         0.00000000e+00, 9.25924671e-01, 2.42512483e-02],\n",
       "        [6.73499779e-02, 9.82970517e-04, 2.05469846e-02, 1.33480040e-02,\n",
       "         5.54803672e-03, 1.08768414e-01, 7.83455612e-01]]),\n",
       " 'AverageAcc': 0.8917790560347527}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do one prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_latest_p36]",
   "language": "python",
   "name": "conda-env-pytorch_latest_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
