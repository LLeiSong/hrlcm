---
title: "Notebook of learning from ensemble labels"
author: "Lei Song"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown:
    highlight: pygments
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Set up GPU environment
### AWS based method
#### Launch AWS instance
If you decide to use AWS instance, here are some basic steps.
The information of instance that we used:

- AMI ID: `ami-0643df3b347740189`, which is our customized deep learning AWS instance. Or this one `ami-005050e76c11b3b16`, which includes datasets already.
- Instance type: a few instance could use based on different needs.

  - `g3.8xlarge`, 2 GPU, 16G GPU memory, $0.684/hr for SPOT instance.
  - `g3.16xlarge`, 4 GPU, 32G GPU memory, $1.368/hr for SPOT instance.
  - `g4dn.8xlarge`, 1 GPU, 16G GPU memory, $0.6528/hr for SPOT instance.
  - `g4dn.12xlarge`, 4 GPU, 64G GPU memory, $1.1819/hr for SPOT instance.
    
- Security group: `labeller-security`.

Before run this script, do the following steps. Just need step one for Linux machine. Didn't test Windows machine.

1. `brew install awscli`, then `aws configure` to configure AWS credential.
2. `brew install jq`
3. `brew install coreutils`. to use gdata on macOS.

Then:
```
cd tools
chmod 777 create_spot_instance.sh
create_spot_instance.sh <ami_id> <instance_type> <security_group_id> <new_instance_name> <spot_type> <valid_until> <key_name> <volume_size>
```
For example:
```
cd tools
chmod 777 create_spot_instance.sh
create_spot_instance.sh ami-0643df3b347740189 g3.16xlarge sg-0a8bbc91697d6a76b lc_dl_tz persistent 2021-01-22T23:00:00 lsong-keypair 200
```

#### Parameters setting
After successfully launched the instance, then it is ready to get into the instance to work.

The most essential work are arguably to clone project and set jupyter notebook:

- STEP 1: Set up GitHub token (because the repo is private). More details could be found in this [tutorial](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).

  ```
  ssh-keygen -t ed25519 -C "your_email@example.com"
  eval "$(ssh-agent -s)"
  ssh-add ~/.ssh/id_ed25519
  vim ~/.ssh/id_ed25519.pub
  ```
  Copy out the ssh key, and type `:q` to exit, and then go to GitHub page to add into the records.
    
- STEP 2: Clone the repo into instance:
  
  ```
  git clone git@github.com:agroimpacts/pytorch_planet.git
  ```
  
  If necessary, use `git checkout` to switch branch and use `git pull` to grab the updates.
  
- STEP 3: Set up jupyter notebook

  ```
  jupyter notebook password
  ```
  to set a log in password for jupyter notebook, then
  ```
  jupyter notebook
  ```
  replace the private IP with the public IP to open it in browser. If you can log in without issues, then it is ready to go.
  
#### Tensorboard

If download log file to local machine, then it is easy to check the training loss curves.

`tensorboard --logdir='./tensorboard_dirs' --port=16007`

Do the following steps to use the tensorboard on a remote server (reference: https://blog.yyliu.net/remote-tensorboard/):

1. You need to start SSH with transfer the remote serverâ€™s port to your local PC:

    (on your local PC) `ssh -L 16007:127.0.0.1:16007 username@server_ip`

2. Then you may start the tensorboard on the remote server with the specific port we want to transfer:

    (on the remote server) `tensorboard --logdir='./tensorboard_dirs' --port=16007`

### Google Colab
In general, we mean to use Google Colab to run the model. For this case, it is more convenient to read the data locally and pickle them. Then directly load the pickles into Google Colab and just use the GPU environment to run the models.

## Methodology
### Candidate models

- U-net, which has simple architecture and is friendly to small dataset. This model is especially suitable for guess training.

- Deeplab3plus with ResNet as backbone. This architecture can consider the global context and have skip connections to ensure the complex structure still could get coverage. This model is super useful for the final train with manually corrected labels.

### Strategies to deal with noisy labels
#### Label smoothing
The general loss is higher than classic cross entropy loss. But it do help to reduce the gaps between train and validate. So it might be a promising method to use for our case. There is one general parameter "smooth rate". Based on the experiments, a higher smooth rate could help to reduce the gap between train and validate more. 0.5 is a safe choice.

#### Co-teaching and co-teaching-plus



#### JoCoR



## Experiments

### Single model
Before any tests related to strategies to deal with noisy labels, we will build up a baseline model only using "pure" labels generated by human checking.

#### Hyper-parameters

- Learning rate and learning rate scheduler
- Batch size
- Optimizer
- Model (Unet or Deeplab)
- Transform methods (if necessary)

#### Tuning plans

- Unet

  - Batch size = 32 (Maximum value fit into `g4dn.12xlarge`)
  - Initial learning rate = [0.1, 0.01, 0.001]
  
    - 0.1 is too large
    - 0.01 and 0.001 works similarly.
  
  - Optimizer: we plan to try some more recent optimizer such as AdaBound, AdamP, AdamW, etc. Because SGD could make model have higher generalization while Adam can train the model faster. So if we could use an optimizer that has both advantages, it would be ideal. Since our research is not about the improvement of DL algorithm, so we just pick up the most right one at the moment.
  
  - Scheduler: 0 - 50 use 0.01, 50 - 150 use a decayed cyclic scheduler (0.01 - 0.001), last 50: 0.001 with a tiny cycle range.

### Double model
