---
title: "Notebook of learning from ensemble labels"
author: "Lei Song"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown:
    highlight: pygments
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Set up GPU environment
### AWS based method
#### Launch AWS instance
If you decide to use AWS instance, here are some basic steps.
The information of instance that we used:

- AMI ID: `ami-0643df3b347740189`, which is our customized deep learning AWS instance. Or this one `ami-005050e76c11b3b16`, which includes datasets already.
- Instance type: a few instance could use based on different needs.

  - `g3.8xlarge`, 2 GPU, 16G GPU memory, $0.684/hr for SPOT instance.
  - `g3.16xlarge`, 4 GPU, 32G GPU memory, $1.368/hr for SPOT instance.
  - `g4dn.8xlarge`, 1 GPU, 16G GPU memory, $0.6528/hr for SPOT instance.
  - `g4dn.12xlarge`, 4 GPU, 64G GPU memory, $1.1819/hr for SPOT instance.
    
- Security group: `labeller-security`.

Before run this script, do the following steps. Just need step one for Linux machine. Didn't test Windows machine.

1. `brew install awscli`, then `aws configure` to configure AWS credential.
2. `brew install jq`
3. `brew install coreutils`. to use gdata on macOS.

Then:
```
cd tools
chmod 777 create_spot_instance.sh
create_spot_instance.sh <ami_id> <instance_type> <security_group_id> <new_instance_name> <spot_type> <valid_until> <key_name> <volume_size>
```
For example:
```
cd tools
chmod 777 create_spot_instance.sh
create_spot_instance.sh ami-0643df3b347740189 g3.16xlarge sg-0a8bbc91697d6a76b lc_dl_tz persistent 2021-01-22T23:00:00 lsong-keypair 200
```

#### Instance setting
After successfully launched the instance, then it is ready to get into the instance to work.

The most essential work are arguably to clone project and set jupyter notebook:

- STEP 1: Set up GitHub token (because the repo is private). More details could be found in this [tutorial](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).

  ```
  ssh-keygen -t ed25519 -C "your_email@example.com"
  eval "$(ssh-agent -s)"
  ssh-add ~/.ssh/id_ed25519
  vim ~/.ssh/id_ed25519.pub
  ```
  Copy out the ssh key, and type `:q` to exit, and then go to GitHub page to add into the records.
    
- STEP 2: Clone the repo into instance:
  
  ```
  git clone git@github.com:agroimpacts/pytorch_planet.git
  ```
  
  If necessary, use `git checkout` to switch branch and use `git pull` to grab the updates.
  
- STEP 3: Set up jupyter notebook

  ```
  jupyter notebook password
  ```
  to set a log in password for jupyter notebook, then
  ```
  jupyter notebook
  ```
  replace the private IP with the public IP to open it in browser. If you can log in without issues, then it is ready to go.
  
#### Run experiments

```
python hrlcm/fit_coteaching_compare.py --exp_name unet_cot_argue --num_workers 32 --gpu_devices '0,1,2,3' --epochs 30
```

```
python hrlcm/fit_coteaching.py --exp_name unet_cot_da --num_workers 32 --gpu_devices '0,1,2,3' --save_freq 5 --resume1 'results/dl/unet_cot_da/model1_final.pth' --resume2 'results/dl/unet_cot_da/model2_final.pth'
```
  
#### Tensorboard

If download log file to local machine, then it is easy to check the training loss curves.

`tensorboard --logdir='./tensorboard_dirs' --port=16007`

Do the following steps to use the tensorboard on a remote server (reference: https://blog.yyliu.net/remote-tensorboard/):

1. You need to start SSH with transfer the remote serverâ€™s port to your local PC:

    (on your local PC) `ssh -L 16007:127.0.0.1:16007 username@server_ip`

2. Then you may start the tensorboard on the remote server with the specific port we want to transfer:

    (on the remote server) `tensorboard --logdir='./tensorboard_dirs' --port=16007`

### Google Colab
In general, we mean to use Google Colab to run the model. For this case, it is more convenient to read the data locally and pickle them. Then directly load the pickles into Google Colab and just use the GPU environment to run the models.

## Methodology
### Candidate models

- U-net, which has simple architecture and is friendly to small dataset. This model is especially suitable for guess training.

- Deeplab3plus with ResNet as backbone. This architecture can consider the global context and have skip connections to ensure the complex structure still could get coverage. This model is super useful for the final train with manually corrected labels.

### Strategies to deal with noisy labels

#### Add noisy labels to fine tune

After the initial 200 epochs with perfect labels, adding more noisy labels to "adapt" the model. 

#### Label smoothing

The general loss is higher than classic cross entropy loss. But it do help to reduce the gaps between train and validate. So it might be a promising method to use for our case. There is one general parameter "smooth rate". Based on the experiments, a higher smooth rate could help to reduce the gap between train and validate more. 0.5 is a safe choice.

#### Co-teaching and co-teaching-plus

Based on the original papers and their GitHub repos, we combined these two logic into one function and adapted it for image segmentation. 

- The first version is image based, which means it takes each image as a whole thing. If the label of one image is "perfect", then use the whole image, otherwise only use pixels with small loss to update the models.
  - Under "argue" mode, use disagreement to update the models.
  - Under "discuss" mode, do not use disagreement.
  
- The second version is batch based, which means it only use pixels with small loss from the whole batch to update the models. Mode works similarly as the first version. 

#### JoCoR
Since the original paper is about image classification rather than image segmentation, it is harder to adapt this method into image segmentation with good performance. 

[NEED MORE EXPERIMENTS]

## Experiments

### Start up (Baseline)

Before any tests related to strategies to deal with noisy labels, we will build up a baseline model only using "pure" labels generated by human checking.

#### Hyper-parameters

- <input type="checkbox" checked> Learning rate and learning rate scheduler</input>
- Batch size
- Optimizer
- Model (U-net or Deeplab)
- Transform methods (if necessary)

##### Unet

- Batch size = 32 (Maximum value fit into `g4dn.12xlarge`)
- Initial learning rate = [0.1, 0.01, 0.001]
  
  - 0.1 is too large
  - 0.01 and 0.001 works similarly, but 0.001 works slightly better for multiple optimizer.
  - Learning rate smaller than 0.0001 might be too small for this case.
  
- Optimizer: we plan to try some more recent optimizer such as AdaBound, AdamP, etc. Because SGD could make model have higher generalization while Adam can train the model faster. So if we could use an optimizer that has both advantages, it would be ideal. AdaBound is such kind of optimizer. Since our research is not about the improvement of DL algorithm, so we just pick up the most right one at the moment.
  
  - AdaBound does obviously better than Adam or AdamW at least in the first 20 epochs.
  - AdamP performs similar to AdaBound, that are much better than other traditional ones. But validate loss is a lit higher than AdaBound.
  
- Scheduler: we decided to use a compose scheduler to have dynamic learning rate. Even though Adam-based optimizer uses dynamic learning rate already, but experiments prove that using both dynamic learning rate and Adam-based optimizer together could have better performance. So we chose to use 0.001 for the first 30 epochs, to use a decayed cyclic scheduler (0.0008 - 0.0012) for epoch 30 - 120, and use another decayed cyclic scheduler (0.0004 - 0.0006) for last 80 epochs.
  
##### DeepLab

  [TO BE CONTINUE]

### Co-teaching

We first use constant parameters, 0.001 learning rate, AmsBound optimizer, and 30 epochs, to run the models.

- Co_teaching_batch doesn't fit multi-classes classification task. It might be good for binary classification. Because based on the curves, co_teaching_batch give up the minorities. 

- Co_teaching works good. "Discuss" mode works slightly better than "argue" mode.
