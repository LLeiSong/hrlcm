---
title: "Notebook of learning from ensemble labels"
author: "Lei Song"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown:
    highlight: pygments
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Set up GPU environment
### AWS based method
#### Launch AWS instance
If you decide to use AWS instance, here are some basic steps.
The information of instance that we used:

- AMI ID: `ami-0643df3b347740189`, which is our customized deep learning AWS instance. Or this one `ami-005050e76c11b3b16`, which includes datasets already.
- Instance type: a few instance could use based on different needs.

  - `g3.8xlarge`, 2 GPU, 16G GPU memory, $0.684/hr for SPOT instance.
  - `g3.16xlarge`, 4 GPU, 32G GPU memory, $1.368/hr for SPOT instance.
  - `g4dn.8xlarge`, 1 GPU, 16G GPU memory, $0.6528/hr for SPOT instance.
  - `g4dn.12xlarge`, 4 GPU, 64G GPU memory, $1.1819/hr for SPOT instance.
    
- Security group: `labeller-security`.

Before run this script, do the following steps. Just need step one for Linux machine. Didn't test Windows machine.

1. `brew install awscli`, then `aws configure` to configure AWS credential.
2. `brew install jq`
3. `brew install coreutils`. to use gdata on macOS.

Then:
```
cd tools
chmod 777 create_spot_instance.sh
create_spot_instance.sh <ami_id> <instance_type> <security_group_id> <new_instance_name> <spot_type> <valid_until> <key_name> <volume_size>
```
For example:
```
cd tools
chmod 777 create_spot_instance.sh
create_spot_instance.sh ami-0643df3b347740189 g3.16xlarge sg-0a8bbc91697d6a76b lc_dl_tz persistent 2021-01-22T23:00:00 lsong-keypair 200
```

#### Instance setting
After successfully launched the instance, then it is ready to get into the instance to work.

The most essential work are arguably to clone project and set jupyter notebook:

- STEP 1: Set up GitHub token (because the repo is private). More details could be found in this [tutorial](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).

  ```
  ssh-keygen -t ed25519 -C "your_email@example.com"
  eval "$(ssh-agent -s)"
  ssh-add ~/.ssh/id_ed25519
  vim ~/.ssh/id_ed25519.pub
  ```
  Copy out the ssh key, and type `:q` to exit, and then go to GitHub page to add into the records.
    
- STEP 2: Clone the repo into instance:
  
  ```
  git clone git@github.com:LLeiSong/hrlcm.git
  ```
  
  If necessary, use `git checkout` to switch branch and use `git pull` to grab the updates.
  
- STEP 3: Set up jupyter notebook

  ```
  jupyter notebook password
  ```
  to set a log in password for jupyter notebook, then
  ```
  jupyter notebook
  ```
  replace the private IP with the public IP to open it in browser. If you can log in without issues, then it is ready to go.
  
#### Run experiments

Prepare work:

```
scp -r /Volumes/wildebeest/dl_train ubuntu@ec2-3-19-239-231.us-east-2.compute.amazonaws.com:~/hrlcm/results/north

scp -r /Volumes/wildebeest/dl_valid ubuntu@ec2-3-19-239-231.us-east-2.compute.amazonaws.com:~/hrlcm/results/north
```

Run models:

```
python hrlcm/fit_step_lr.py --exp_name unet_one_tile --num_workers 32 --gpu_devices '0,1,2,3'
```

```
python hrlcm/fit_compose_lr.py --exp_name unet_k15_hard12_clr_200epc --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --epochs 201
```

- Weights

Score
```
python hrlcm/fit_step_lr.py --exp_name unet_no_weight --quality_weight 0 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; python hrlcm/fit_step_lr.py --exp_name unet_k05_only --quality_weight 1 --score_k 0.5 --hardiness_max 1 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; python hrlcm/fit_step_lr.py --exp_name unet_k15_only --quality_weight 1 --score_k 1.5 --hardiness_max 1 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; python hrlcm/fit_step_lr.py --exp_name unet_k3_only --quality_weight 1 --score_k 3.0 --hardiness_max 1 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31
```

Hardiness
```
python hrlcm/fit_step_lr.py --exp_name unet_k15_hard11 --quality_weight 1 --score_k 1.5 --hardiness_max 1.1 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; python hrlcm/fit_step_lr.py --exp_name unet_k15_hard12 --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; python hrlcm/fit_step_lr.py --exp_name unet_k15_hard14 --quality_weight 1 --score_k 1.5 --hardiness_max 1.4 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31
```

```
python hrlcm/predict.py --args_path 'results/dl/unet_k3_only/checkpoints/args.pkl' --checkpoint_file 'results/dl/unet_k3_only/checkpoints/final.pth' --fname_predict 'dl_catalog_test.csv' --data_dir 'results/north' --out_dir 'results/prediction' --stats_dir 'results/north/norm_stats' --num_workers 32 --batch_size 32 --gpu_devices '0,1,2,3'
```

- Sampling

```
scp /Users/pinot/Dropbox/research/hrlcm/results/north/dl_catalog_train_1.csv ubuntu@ec2-3-16-90-184.us-east-2.compute.amazonaws.com:~/hrlcm/results/north;scp /Users/pinot/Dropbox/research/hrlcm/results/north/dl_catalog_train_2.csv ubuntu@ec2-3-16-90-184.us-east-2.compute.amazonaws.com:~/hrlcm/results/north;scp /Users/pinot/Dropbox/research/hrlcm/results/north/dl_catalog_train_random_34.csv ubuntu@ec2-3-16-90-184.us-east-2.compute.amazonaws.com:~/hrlcm/results/north;scp /Users/pinot/Dropbox/research/hrlcm/results/north/dl_catalog_train_random_67.csv ubuntu@ec2-3-16-90-184.us-east-2.compute.amazonaws.com:~/hrlcm/results/north;scp /Users/pinot/Dropbox/research/hrlcm/results/north/dl_catalog_train_hori_strip_34.csv ubuntu@ec2-3-16-90-184.us-east-2.compute.amazonaws.com:~/hrlcm/results/north;scp /Users/pinot/Dropbox/research/hrlcm/results/north/dl_catalog_train_hori_strip_67.csv ubuntu@ec2-3-16-90-184.us-east-2.compute.amazonaws.com:~/hrlcm/results/north;scp /Users/pinot/Dropbox/research/hrlcm/results/north/dl_catalog_train_vert_strip_34.csv ubuntu@ec2-3-16-90-184.us-east-2.compute.amazonaws.com:~/hrlcm/results/north;scp /Users/pinot/Dropbox/research/hrlcm/results/north/dl_catalog_train_vert_strip_67.csv ubuntu@ec2-3-16-90-184.us-east-2.compute.amazonaws.com:~/hrlcm/results/north
```

```
python hrlcm/fit_step_lr.py --exp_name unet_one_tile --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; rm results/north/dl_catalog_train.csv; mv results/north/dl_catalog_train_random_34.csv results/north/dl_catalog_train.csv; python hrlcm/fit_step_lr.py --exp_name unet_random_34 --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; rm results/north/dl_catalog_train.csv; mv results/north/dl_catalog_train_hori_strip_34.csv results/north/dl_catalog_train.csv; python hrlcm/fit_step_lr.py --exp_name unet_hori_strip_34 --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; rm results/north/dl_catalog_train.csv; mv results/north/dl_catalog_train_vert_strip_34.csv results/north/dl_catalog_train.csv; python hrlcm/fit_step_lr.py --exp_name unet_vert_strip_34 --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; rm results/north/dl_catalog_train.csv; mv results/north/dl_catalog_train_2.csv results/north/dl_catalog_train.csv; python hrlcm/fit_step_lr.py --exp_name unet_two_tile --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; rm results/north/dl_catalog_train.csv; mv results/north/dl_catalog_train_random_67.csv results/north/dl_catalog_train.csv; python hrlcm/fit_step_lr.py --exp_name unet_random_67 --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; rm results/north/dl_catalog_train.csv; mv results/north/dl_catalog_train_hori_strip_67.csv results/north/dl_catalog_train.csv; python hrlcm/fit_step_lr.py --exp_name unet_hori_strip_67 --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31; rm results/north/dl_catalog_train.csv; mv results/north/dl_catalog_train_vert_strip_67.csv results/north/dl_catalog_train.csv; python hrlcm/fit_step_lr.py --exp_name unet_vert_strip_67 --quality_weight 1 --score_k 1.5 --hardiness_max 1.2 --num_workers 32 --gpu_devices '0,1,2,3' --max_lr 0.0001 --gamma_lr 1 --epochs 31
```

#### Tensorboard

If download log file to local machine, then it is easy to check the training loss curves.

`tensorboard --logdir='./tensorboard_dirs' --port=16007`

Do the following steps to use the tensorboard on a remote server (reference: https://blog.yyliu.net/remote-tensorboard/):

1. You need to start SSH with transfer the remote serverâ€™s port to your local PC:

    (on your local PC) `ssh -L 16007:127.0.0.1:16007 username@server_ip`

2. Then you may start the tensorboard on the remote server with the specific port we want to transfer:

    (on the remote server) `tensorboard --logdir='./tensorboard_dirs' --port=16007`
    
#### Prediction

```
python hrlcm/predict.py --args_path 'results/dl/unet_k15_hard12_clr_200epc/checkpoints/args.pkl' --checkpoint_file 'results/dl/unet_k15_hard12_clr_200epc/checkpoints/final.pth' --fname_predict 'dl_catalog_predict.csv' --data_dir 'results/north' --out_dir 'results/prediction' --stats_dir 'results/north/norm_stats' --num_workers 32 --batch_size 32 --gpu_devices '0,1,2,3'

scp -r ubuntu@ec2-18-191-228-234.us-east-2.compute.amazonaws.com:~/hrlcm/results/prediction /Users/pinot/Dropbox/research/hrlcm/results/dl/unet_score08_hardiness2_epoch200
```

#### Evaluation

```
scp /Users/pinot/Dropbox/research/hrlcm/results/dl/unet_score08_hardiness2_epoch200/checkpoints/final.pth ubuntu@ec2-18-191-228-234.us-east-2.compute.amazonaws.com:~/hrlcm/results/dl/final.pth;
scp /Users/pinot/Dropbox/research/hrlcm/results/dl/unet_score08_hardiness2_epoch200/checkpoints/args.pkl ubuntu@ec2-18-191-228-234.us-east-2.compute.amazonaws.com:~/hrlcm/results/dl/args.pkl

python hrlcm/evaluate.py --args_path 'results/dl/unet_k15_hard12_clr_200epc/checkpoints/args.pkl' --checkpoint_file 'results/dl/unet_k15_hard12_clr_200epc/checkpoints/final.pth' --data_dir 'results/north' --out_dir 'results/evaluation' --stats_dir 'results/north/norm_stats' --num_workers 32 --batch_size 32 --gpu_devices '0,1,2,3'

scp -r ubuntu@ec2-18-191-228-234.us-east-2.compute.amazonaws.com:~/hrlcm/results/evaluation /Users/pinot/Dropbox/research/hrlcm/results/dl/unet_score08_hardiness2_epoch200
```

#### Other useful commands

##### Check disk space and clean trash

```bash
df -H
cd ~/.local/share/Trash/
rm -rf *
```
##### Check the RAM usage in M

```bash
free -m
```

## Methodology
### Candidate models

- U-net, which has simple architecture and is friendly to small dataset.

### Possible strategies to deal with noisy labels
#### Add noisy labels to fine tune

After the initial 200 epochs with perfect labels, adding more noisy labels to "adapt" the model. 

#### Label smoothing

The general loss is higher than classic cross entropy loss. But it do help to reduce the gaps between train and validate. So it might be a promising method to use for our case. There is one general parameter "smooth rate". Based on the experiments, a higher smooth rate could help to reduce the gap between train and validate more. 0.5 is a safe choice.

#### Co-teaching and co-teaching-plus

Based on the original papers and their GitHub repos, we combined these two logic into one function and adapted it for image segmentation. 

- The first version is image based, which means it takes each image as a whole thing. If the label of one image is "perfect", then use the whole image, otherwise only use pixels with small loss to update the models.
  - Under "argue" mode, use disagreement to update the models.
  - Under "discuss" mode, do not use disagreement.
  
- The second version is batch based, which means it only use pixels with small loss from the whole batch to update the models. Mode works similarly as the first version. 

#### JoCoR

Since the original paper is about image classification rather than image segmentation, it is harder to adapt this method into image segmentation with good performance.

So after all experiments, we decide to stick with co-teaching & co-teaching-plus.

## Experiments

### Start up (Baseline)

Before any tests related to strategies to deal with noisy labels, we will build up a baseline model only using "pure" labels generated by human checking.

#### Hyper-parameters

- <input type="checkbox" checked> Learning rate and learning rate scheduler</input>
- <input type="checkbox" checked> Batch size</input>
- <input type="checkbox" checked> Optimizer</input>
- <input type="checkbox" checked> Model (U-net or Deeplab)</input>
- Transform methods (if necessary)

##### Unet

- Batch size = 32 (Maximum value fit into `g4dn.12xlarge`)
- Initial learning rate = [0.1, 0.01, 0.001]
  
  - 0.1 is too large
  - 0.01 and 0.001 works similarly, but 0.001 works slightly better for multiple optimizer.
  - Learning rate smaller than 0.0001 might be too small for this case.
  
- Optimizer: we plan to try some more recent optimizer such as AdaBound, AdamP, etc. Because SGD could make model have higher generalization while Adam can train the model faster. So if we could use an optimizer that has both advantages, it would be ideal. AdaBound is such kind of optimizer. Since our research is not about the improvement of DL algorithm, so we just pick up the most right one at the moment.
  
  - AdaBound does obviously better than Adam or AdamW at least in the first 20 epochs.
  - AdamP performs similar to AdaBound, that are much better than other traditional ones. But validate loss is a lit higher than AdaBound.
  
- Scheduler: we decided to use a compose scheduler to have dynamic learning rate. Even though Adam-based optimizer uses dynamic learning rate already, but experiments prove that using both dynamic learning rate and Adam-based optimizer together could have better performance. So we chose to use 0.001 for the first 30 epochs, to use a decayed cyclic scheduler (0.0008 - 0.0012) for epoch 30 - 120, and use another decayed cyclic scheduler (0.0004 - 0.0006) for last 80 epochs.
  
##### DeepLab

  Our experiments proved that architecture of DeepLab family has worse performance.

### Co-teaching

We first use constant parameters, 0.001 learning rate, AmsBound optimizer, and 30 epochs, to run the models.

- Co_teaching_batch doesn't fit multi-classes classification task. It might be good for binary classification. Because based on the curves, co_teaching_batch give up the minorities. 

- Co_teaching works good. "Discuss" mode works slightly better than "argue" mode. Since "argue" mode use the disagreement between two models, we assume this method would be helpful to transfer a model to other areas. Of course, this assumption needs more test.


So now, I have a clear idea is that the "wrong" guess labels are actually the hard ones. So instead of dumping them, we should take advantage of them.

- Randomly get 8 out of 64 for each tile, 6 for train, and 2 for validate.
- For these sub-tiles, score them and also correct the wrong part. The scores will be used as weight to calculate loss. The lower the score is, the higher weight it is.
