---
title: "Make the full map of Tanzania land cover"
author: "Lei Song"
date: "1/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Introduction

This document is to record steps for making the full map of Tanzania.

- Generate guessing labels (done)
- Manually check labels (done)
- Train U-Net model for the whole country

## Steps
### Generate guessing labels

Prepare the needed datasets and install necessary software (e.g. GRASS GIS) and libraries. Run the following scripts under guess_model to get the guessed labels:

- 0_set_up.R
- 1_ensemble_labels.R
- 2_get_osm.R
- 3_update_lc_labels_osm.R
- 4_sampling.R
- 5_get_training.R
- 6_train_guess_model.R
- 7_pred_guess_model.R

### Manually check labels

Analyze the reliability of labels based on the score of guessed label and the quality of satellite images.

#### Check the quality of labels

```{r}
tiles <- read_sf("results/tanzania/catalog_sample_tiles.geojson")
library(tidyr)
# Check labels
stats_guess_labels <- do.call(
    rbind, 
    lapply(unique(tiles$tile), 
           function(tile_id){
               sub_tiles <- tiles %>% filter(tile == tile_id) %>% 
                   st_drop_geometry() %>% 
                   mutate(surfix = paste(tile, index, sep = '_')) %>% 
                   mutate(score_path = file.path(here('results/tanzania/guess_labels'),
                                                 sprintf('score_%s.tif', surfix)))
               do.call(rbind, lapply(sub_tiles$surfix, function(sur) {
                   pth <- sub_tiles %>% filter(surfix == sur) %>% 
                       pull(score_path)
                   scores <- values(rast(pth))
                   na_ratio <- sum(is.na(scores))/ length(scores)
                   data.frame(surfix = sur,
                              hasna = any(is.na(scores)), 
                              na_ratio = na_ratio,
                              mean = mean(scores, na.rm = T),
                              sd = sd(scores, na.rm = T),
                              min = min(scores, na.rm = T),
                              max = max(scores, na.rm = T))}))
               }
           )
    )
save(stats_guess_labels, file = 'results/tanzania/stats_guess_labels.rda')
err_labels <- stats_guess_labels %>% filter(hasna == TRUE) %>% 
    separate(col = surfix, into = c('tile', 'index'), sep = "_")
```

#### Check the quality of images

```{r}
# Check images
stack_dir <- '/Volumes/elephant/pred_stack'
img_quality <- do.call(
    rbind, 
    mclapply(unique(tiles$tile), 
             function(tile_id){
                 imgs <- rast(file.path(stack_dir, paste0(tile_id, '.tif')))
                 nas <- is.na(imgs)
                 zeros <- imgs == 0
                 rm(imgs)
                 nas <- freq(nas) %>% as.data.frame() %>% 
                     mutate(count = count / (dim(nas)[1] * dim(nas)[2])) %>% 
                     rename(na_ratio = count) %>% 
                     mutate(tile_id = tile_id) %>% 
                     filter(value == 1) %>% select(-value)
                 zeros <- freq(zeros) %>% as.data.frame() %>% 
                     mutate(count = count / (dim(zeros)[1] * dim(zeros)[2])) %>% 
                     rename(zero_ratio = count) %>% 
                     mutate(tile_id = tile_id) %>% 
                     filter(value == 1) %>% select(-value)
                 gc()
                 full_join(nas, zeros, by = c('tile_id', 'layer')) %>% 
                     select(tile_id, layer, na_ratio, zero_ratio)
             }, mc.cores = 6))

imgs <- rast(file.path(stack_dir, paste0(tiles$tile[1], '.tif')))
layers <- data.frame(layer_name = names(imgs)) %>% 
    mutate(layer = 1:nrow(.))
img_quality <- left_join(img_quality, layers, by = 'layer') %>% 
    select(tile_id, layer_name, na_ratio, zero_ratio); rm(imgs, layers)
save(img_quality, file = 'results/tanzania/img_quality.rda')
img_withna <- img_quality %>% filter(!is.na(na_ratio)) %>% 
    pull(tile_id) %>% unique()
img_withzero <- img_quality %>% filter(zero_ratio > 0.3) %>% 
    filter(!grepl("vv|vh", layer_name)) %>% 
    pull(tile_id) %>% unique()
```

[*NOTE:*] There are a few problematic images. Skip them for now. These tiles can be fixed after segmentation. 

```{r}
# 1192-988, water, missing partial image
# 1207-971, land, missing half image
# 1229-975, land, seems weird
# 1234-957, land, missing the whole image
# 1246-962, land, missing the whole image
# VH
# 1200-974, land and water, seems weird
# 1199-1018, land, seems weird
# 1245-974, land, a small fragment seems weird
```

#### Get candidate validation and train

Get one sub tile from each tile randomly as validation. Not all of them will be used. After manually checking, low quality ones that cannot be fixed easily would be excluded.

Why random ones rather than good ones? To optimize the cooperation of random forest and U-Net. Because if selecting good ones, the results might be biased to easy-to-model instances.

```{r}
# Validation
skip_tile <- err_labels %>% pull(tile) %>% unique()
stats_guess_labels <- stats_guess_labels %>% 
    separate(col = surfix, into = c('tile', 'index'), sep = "_") %>% 
    filter(!tile %in% skip_tile)
set.seed(124)
validate_labels <- stats_guess_labels %>% 
    group_by(tile) %>% 
    sample_n(1) %>% ungroup() %>% 
    dplyr::select(tile, index) %>% 
    mutate(index = as.integer(index))

# Check the validate
# 1 is TRUE, 0 is FALSE
tiles_validate <- right_join(tiles, validate_labels, by = c("tile", "index")) %>% 
    mutate(use = 1, modify = 0)
plot(tiles_validate$geometry)
write_sf(tiles_validate, "results/tanzania/catalog_tiles_validate.geojson")
```

```{r}
# Train
tile_index_val <- tiles_validate %>% 
    mutate(tile_index = paste(tile, index, sep = "_")) %>% 
    pull(tile_index)
tile_index_train <- stats_guess_labels %>% 
    mutate(index = as.integer(index)) %>% 
    mutate(tile_index = paste(tile, index, sep = "_")) %>% 
    filter(!tile_index %in% tile_index_val) %>% 
    dplyr::select(tile, index)
tiles_train <- right_join(tiles, tile_index_train, by = c("tile", "index")) %>% 
    mutate(use = 1, modify = 0, comment = NA)
write_sf(tiles_train, "results/tanzania/catalog_tiles_train.geojson")
```

#### Manually check

Next, the author manually checked sub-tiles of validation and train. Script `8_human_refine.R` can help to do this step. 

#### Add existing labels of north to use

Add to catalogs

```{r}
# Get labels from north
## Randomly get 50% tiles
## Get two for training

# Read train labels
num_tiles <- read_sf(here("data/geoms/tiles_nicfi_north.geojson")) %>% nrow()
north_train <- read.csv(here("results/north/dl_catalog_train.csv"),
  stringsAsFactors = F)

# Keep two for each tile
north_train <- do.call(
  rbind,
  lapply(
    unique(north_train$tile),
    function(this_tile) {
      north_train %>%
        filter(tile == this_tile) %>%
        arrange(desc(score)) %>%
        slice(1:2)}))

# Read validate labels
north_valid <- read.csv(here("results/north/dl_catalog_valid.csv"),
  stringsAsFactors = F)

# Randomly select 50% of tile to use
set.seed(10)
tile_to_use <- north_train %>%
  group_by(tile) %>%
  summarise(score = sum(score)) %>%
  filter(score == 10) %>%
  sample_n(round(num_tiles * 0.5)) %>%
  pull(tile)

# Finalize train and validate
train_path <- 'train'
valid_path <- 'validation'
north_train <- north_train %>%
  filter(tile %in% tile_to_use) %>%
    mutate(
    label = file.path(
      train_path,
      paste0(tile, "_", index, "_label.tif")
    ),
    img = file.path(
      train_path,
      paste0(tile, "_", index, "_img.tif")
    )) %>% 
  dplyr::select(tile_id, tile, index, label, img)
north_valid <- north_valid %>%
  filter(tile %in% tile_to_use) %>%
    mutate(
    label = file.path(
      valid_path,
      paste0(tile, "_", index, "_label.tif")
    ),
    img = file.path(
      valid_path,
      paste0(tile, "_", index, "_img.tif")
    )) %>% 
  dplyr::select(tile_id, tile, index, label, img)

# Add to catalog of full country
catalog_train <- read.csv(here("results/tanzania/dl_catalog_train.csv"),
                          stringsAsFactors = FALSE) %>% 
    rbind(north_train)
catalog_validate <- read.csv(here("results/tanzania/dl_catalog_valid.csv"),
                             stringsAsFactors = FALSE) %>% 
    rbind(north_valid)
write.csv(catalog_train, 
          here('results/tanzania/dl_catalog_train.csv'),
          row.names = F)
write.csv(catalog_validate, 
          here('results/tanzania/dl_catalog_valid.csv'),
          row.names = F)
```

Double check

```{r}
catalog_train <- read.csv(here("results/tanzania/dl_catalog_train.csv"),
                          stringsAsFactors = FALSE)
files_should <- c(catalog_train$label, catalog_train$img)
files_have <- file.path("train", list.files(here("results/tanzania/train")))
setdiff(files_should, files_have)

catalog_validate <- read.csv(here("results/tanzania/dl_catalog_valid.csv"),
                             stringsAsFactors = FALSE)
files_should <- c(catalog_validate$label, catalog_validate$img)
files_have <- file.path("validation", list.files(here("results/tanzania/validation")))
setdiff(files_should, files_have)
```

Move images

```{r}
move_train <- lapply(1:nrow(north_train), function(n) {
    things_to <- unlist(north_train[n, c("label", 'img')])
    things_from <- gsub("train", "dl_train", things_to)
    things_to <- file.path(here("results/tanzania"), things_to)
    things_from <- file.path(here("results/north"), things_from)
    file.copy(things_from, things_to)
})

move_valid <- lapply(1:nrow(north_valid), function(n) {
    things_to <- unlist(north_valid[n, c("label", 'img')])
    things_from <- gsub("validation", "dl_valid", things_to)
    things_to <- file.path(here("results/tanzania"), things_to)
    things_from <- file.path(here("results/north"), things_from)
    file.copy(things_from, things_to)
})
```

### Train U-Net model for the whole country

The scripts used for training U-Net are under directory hrlcm. Multiple type of models will be trained:

- Using all bands and image standardization
- Using all bands without image standardization
- Using only NICFI bands with image standardization
- Using only NICFI bands without image standardization

Please reference to tutorial [Introduction of using AWS instance to run experiments](run_experiments.Rmd) for how to run experiments.

