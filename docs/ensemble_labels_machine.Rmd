---
title: "Ensemble labels"
author: "Lei Song"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown:
    highlight: pygments
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Introduction
In order to make full use of other existing land cover (LC) products, and save the energy to gather new labels. We decided to ensemble these chosen LC products to make new labels for training. We chose these products based upon their relatively high spatial resolution and recent produce time. We visually checked the performance of the products, some of them don't make sense even though they might have good classification accuracy. This mainly because these maps are optimized to a much bigger area.

These products include:

- Finer Resolution Observation and Monitoring of Global Land Cover (FROM-GLC10) 2017 (10 m)
- Global Food Security-support Analysis Data (GFSAD) cropland extent of Africa 2015 (30 m)
- Copernicus global land cover map 2018 (100 m) (We chose it because of the most recent year.)

This project provides a practical framework to rapidly generate regional high resolution land cover map on demand. Due to the spatial limitation of Planet NICFI basemap, We only focus on sub-Saharan Africa and take Tanzania as a case study.

## Week LC labels
First, we redefined LC types to ensure that different products match each other. 
Then, we ensemble all LC products to generate LC type labels, and resample to the highest resolution.

### Read related vectors
```{r read}
library(dplyr)
library(terra)
library(sf)
tiles <- vect('data/geoms/tiles_nicfi.geojson')
```

### Mosaic and clip the products
```{r mosaic}
ext_dir <- '/Volumes/elephant/landcovers'

# FROM_GLC
fnames <- list.files(file.path(ext_dir, 
                               'FROM_GLC_2017'),
                     full.names = T)
fromglc <- do.call(merge, lapply(fnames, function(fname){
    rast(fname)
})) %>% crop(tiles) %>% mask(tiles)

# GFSAD30AFCE
fnames <- list.files(file.path(ext_dir, 
                               'GFSAD30AFCE_2015'),
                     full.names = T,
                     pattern = 'tif$')
gfsad <- do.call(merge, lapply(fnames, function(fname){
    rast(fname)
})) %>% crop(tiles) %>% mask(tiles)

# ESAGLC
fnames <- list.files(file.path(ext_dir, 
                               'ESA_GLC_2018'),
                     full.names = T)
esaglc <- do.call(merge, lapply(fnames, function(fname){
    rast(fname)
})) %>% crop(tiles) %>% mask(tiles)
```

### Redefine land cover types

**ESAGLC**
```
0 No input data
111 Closed forest, evergreen needle leaf
113 Closed forest, deciduous needle leaf
112 Closed forest, evergreen, broad leaf
114 Closed forest, deciduous broad leaf
115 Closed forest, mixed
116 Closed forest, unknown
121 Open forest, evergreen needle leaf
123 Open forest, deciduous needle leaf
122 Open forest, evergreen broad leaf
124 Open forest, deciduous broad leaf
125 Open forest, mixed
126 Open forest, unknown
20 Shrubs
30 Herbaceous vegetation
90 Herbaceous wetland
100 Moss and lichen
60 Bare / sparse vegetation
40 Cultivated and managed vegetation / agriculture (cropland)
50 Urban / built up
70 Snow and ice
80 Permanent water bodies
200 Open sea
```

**FROM-GLC**
```
Name	             Code
Cropland	         1
Forest	           2
Grassland	         3
Shrubland	         4
Wetland	           5
Water	             6
Tundra	           7
Impervious surface 8
Bareland	         9
Snow/Ice	         10
```

**GFSAD**
```
Cropland Extent Class Descriptions

Class Label	         Name	           Description
0	                 Water	           Water bodies/no-data
1	                 Non-Cropland	   Non-Cropland areas
2	                 Cropland	       Cropland areas
```

#### Generate convert tables
```{r refine}
# LC type used
lc_types <- data.frame(id = seq(1, 10),
                       name = c('Cropland', 'Forest', 'Grassland', 
                                'Shrubland', 'Wetland', 'Water', 
                                'Tundra', 'Urban/Built up',
                                'Bareland', 'Snow/Ice'))

# Based on our check, open forest classes 
# are closer to Shrubland class for other products,
names <- c('Closed forest, evergreen needle leaf',
           'Closed forest, evergreen, broad leaf',
           'Closed forest, deciduous needle leaf',
           'Closed forest, deciduous broad leaf',
           'Closed forest, mixed',
           'Closed forest, unknown',
           'Open forest, evergreen needle leaf',
           'Open forest, evergreen broad leaf',
           'Open forest, deciduous needle leaf',
           'Open forest, deciduous broad leaf',
           'Open forest, mixed',
           'Open forest, unknown',
           'Shrubs', 'Herbaceous vegetation',
           'Herbaceous wetland', 'Moss and lichen',
           'Bare / sparse vegetation',
           'Cultivated and managed vegetation / agriculture (cropland)',
           'Urban / built up',
           'Snow and ice',
           'Permanent water bodies',
           'Open sea')
esaglc_table <- data.frame(id = c(seq(111, 116), 
                                  seq(121, 126), 
                                  20, 30, 90, 100, 60, 
                                  40, 50, 70, 80, 200),
                           name = names,
                           convert = c(rep(2, 6), rep(4, 7), 
                                       3, 5, 7, 9, 1,
                                       8, 10, 6, 6))

fromglc_table <- data.frame(id = seq(1, 10),
                            name = c('Cropland', 'Forest', 'Grassland',
                                     'Shrubland', 'Wetland', 'Water',
                                     'Tundra', 'Impervious surface',
                                     'Bareland', 'Snow/Ice'),
                            convert = seq(1, 10))

gfsad_table <- data.frame(id = seq(0, 2),
                          name = c('Water',
                                   'Non-Cropland',
                                   'Cropland'),
                          convert = c(6, NA, 1))
```

#### Convert LC maps
```{r convert}
# Path
dst_path <- 'data/landcovers'
if (!dir.exists(dst_path)) dir.create(dst_path)

# ESA-GLC reclassification
esaglc_rclmat <- data.frame(from = esaglc_table$id - 1,
                            to = esaglc_table$id, 
                            becomes = esaglc_table$convert) %>% 
  as.matrix()
esaglc_new <- classify(esaglc, esaglc_rclmat, 
                       right = FALSE, 
                       othersNA = TRUE,
                       filename = file.path(dst_path, 
                                            'esaglc_rcl.tif'),
                       wopt = list(datatype = 'INT1U',
                                   gdal=c("COMPRESS=LZW")))

# FROM-GLC reclassification
fromglc_rclmat <- data.frame(from = fromglc_table$id - 1,
                             to = fromglc_table$id, 
                             becomes = fromglc_table$convert) %>% 
  as.matrix()
fromglc_new <- classify(fromglc, fromglc_rclmat, 
                        right = FALSE, 
                        othersNA = TRUE,
                        filename = file.path(dst_path, 
                                             'fromglc_rcl.tif'),
                        wopt = list(datatype = 'INT1U',
                                    gdal=c("COMPRESS=LZW")))

# GFSAD reclassification
gfsad_rclmat <- data.frame(from = gfsad_table$id - 1,
                           to = gfsad_table$id, 
                           becomes = gfsad_table$convert) %>% 
  as.matrix()
gfsad_new <- classify(gfsad, gfsad_rclmat, 
                      right = FALSE, 
                      othersNA = TRUE,
                      filename = file.path(dst_path, 'gfsad_rcl.tif'),
                      wopt = list(datatype = 'INT1U',
                                  gdal=c("COMPRESS=LZW")))
```

### Make labels
The pixel non-0 value in the image matches with the values in `lc_types`.

```{r make_labels}
# Path
dst_path <- 'data/intermid/lc_labels'
if (!dir.exists(dst_path)) dir.create(dst_path)

# Get the existing types
# Because not all types exist in the current study area
ids <- unique(c(unique(esaglc_new), 
                unique(fromglc_new), 
                1)) # For GFSAD
lc_types <- lc_types %>% filter(id %in% ids)
# Even though there is some pixels classified as snow/ice
# this class is too tiny in Tanzania,
# so we delete this class.
lc_types <- lc_types %>% filter(id != 10)

gfsad_mask <- resample(gfsad_new == 0,
                       fromglc_new, 
                       method = 'near')
lapply(1:nrow(lc_types), function(n){
  lc <- lc_types %>% slice(n)
  message(lc$name)
  if (lc$id == 1){
    message('Just use GFSAD for cropland.')
    lc_lb_emb <- resample(gfsad_new == lc$id, 
                          fromglc_new, 
                          method = 'near')
    gc()
  } else if (lc$id == 6){
    message('Use all three products for water.')
    lc_lb_fromglc <- fromglc_new == lc$id
    lc_lb_esaglc <- resample(esaglc_new == lc$id, 
                             lc_lb_fromglc, 
                             method = 'near')
    lc_lb_gfsad <- resample(gfsad_new == lc$id, 
                            lc_lb_fromglc, 
                            method = 'near')
    lc_lb_emb <- (lc_lb_esaglc + 
                    lc_lb_fromglc + 
                    lc_lb_gfsad) == 3
    rm(lc_lb_esaglc, lc_lb_fromglc, lc_lb_gfsad); gc()
  } else {
    message('Not use GFSAD for other types.')
    lc_lb_fromglc <- fromglc_new == lc$id
    lc_lb_esaglc <- resample(esaglc_new == lc$id, 
                             lc_lb_fromglc,
                             method = 'near')
    # gfsad_mask <- resample(gfsad_new == 0,
    #                        lc_lb_fromglc, 
    #                        method = 'near')
    lc_lb_emb <- (lc_lb_esaglc + 
                    lc_lb_fromglc) == 2
    message('Skip the cropland and water area from gfsad.')
    lc_lb_emb <- lc_lb_emb * gfsad_mask
    rm(lc_lb_esaglc, lc_lb_fromglc); gc()
  }
  
  # Save out
  lc_lb_emb[lc_lb_emb == 1] <- lc$id
  fn <- file.path(dst_path, 
                  paste0(gsub('/| ', '_', 
                              lc$name), 
                         '.tif'))
  writeRaster(lc_lb_emb, fn, 
              wopt = list(datatype = 'INT1U',
                          gdal=c("COMPRESS=LZW")))
  rm(lc_lb_emb);gc()
})
```

#### Labels summary
```{r summary}
labels_sum <- do.call(rbind, 
                      lapply(1:nrow(lc_types), 
                             function(n){
  lc <- lc_types %>% slice(n)
  message(lc$name)
  fn <- file.path(dst_path, 
                  paste0(gsub('/| ', '_', lc$name), 
                         '.tif'))
  lb_sum <- rast(fn) %>% freq()
  label_count <- 0
  label_frac <- 0
  if (nrow(lb_sum) > 1){
    label_count <- lb_sum[2, 3]
  label_frac <- lb_sum[2, 3] / 
    (lb_sum[1, 3] + lb_sum[2, 3])
  }
  data.frame(name = lc$name, 
             count = label_count,
             frac = label_frac)
}))

# Save out
dst_path <- 'data/intermid'
if (!dir.exists(dst_path)) dir.create(dst_path)
write.csv(labels_sum, 
          file.path(dst_path,
                    'labels_sum.csv'),
          row.names = F)
```

```{r show, eval=T, echo=F, message=F, warning=F}
library(here)
library(kableExtra)
dst_path <- 'data/intermid'
labels_sum <- read.csv(here(file.path(dst_path,
                                      'labels_sum.csv')),
                       stringsAsFactors = F)
labels_sum %>%
  kbl(caption = "Ensemble label summary table") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

For Tanzania case, there is no Tundra and Snow/Ice type because Tanzania is a tropical country.

#### Save out the consensus labels
```{r}
dst_path <- 'data/intermid/lc_labels'
labels <- do.call(c, 
                  lapply(1:nrow(lc_types), 
                         function(n){
  lc <- lc_types %>% slice(n)
  message(lc$name)
  fn <- file.path(dst_path, 
                  paste0(gsub('/| ', '_', lc$name), 
                         '.tif'))
  rast(fn)
}))

# Reclassify to make sure common label value is 1
# other value is 2, and background is 0.
labels <- sum(labels)
labels_ext <- labels >= 1
labels_ext <- mask(labels_ext, tiles)
labels_ext[labels_ext == 0] <- 2

# save out
dst_path <- 'data/intermid'
fn <- file.path(dst_path, 'lc_labels.tif')
writeRaster(labels, fn, 
              wopt = list(datatype = 'INT1U',
                          gdal=c("COMPRESS=LZW")))
fn <- file.path(dst_path, 'labels_extent.tif')
writeRaster(labels_ext, fn, 
              wopt = list(datatype = 'INT1U',
                          gdal=c("COMPRESS=LZW")))
```

#### Select out sub-grids
Because of the under representation of classes: Bareland, Wetland, and Built-up, so gather them together as others class to ensure the good classification result. In general, we selected grids under three groups:
- Group 1: Relatively pure class grids (400 * 6 = 2400).
- Group 2: Highlighted areas such as cities and rivers (random 300)
- Group 3: Random grids across the consensus label areas (random 300)

##### Group 1
```{r g1}
library(ggplot2)
library(tidyr)
library(parallel)
# Define the width of chips to cut
# In this case, 256 * 256 chip.
# The chip size is the trade-off between computing cost, 
# purity of ensemble labels, and the keep of geographic context.
width_chips <- 16
lc_labels <- rast('data/intermid/lc_labels.tif')
tiles <- st_read('data/geoms/tiles_nicfi.geojson',
                 quiet = T) %>% dplyr::select(tile)
ecozones <- st_read('data/geoms/ecozones_simple.geojson',
                    quiet = T)

tiles <- do.call(rbind, mclapply(1:nrow(tiles), function(n){
  tile <- tiles %>% slice(n)
  tiles_grids <- st_make_grid(tile, n = c(width_chips, width_chips)) %>% 
    st_sf() %>% arrange() %>% 
    mutate(tile = tile$tile,
           index = c(1:(width_chips^2)))
  do.call(rbind, lapply(1:nrow(tiles_grids), function(m){
    tile <- tiles_grids %>% slice(m)
    ids <- st_intersects(tile, ecozones) %>% unlist()
    tile %>% mutate(ecozone = ids[1])
  }))
}, mc.cores = 12))
tiles <- tiles %>% filter(!is.na(ecozone))
st_write(tiles, 'data/intermid/tiles_grids_ecozones_256.geojson')
tiles <- tiles %>% vect()

tiles_labels <- mclapply(1:nrow(tiles), function(n){
  labels_each <- crop(lc_labels, tiles[n, ])
  lb <- freq(labels_each) %>% data.frame() %>% 
    filter(value != 0) %>%
    mutate(count = count / (dim(labels_each)[1] * dim(labels_each)[2])) %>%
    left_join(., lc_types, by = c('value' = 'id')) %>%
    dplyr::select(-c(value, layer)) %>%
    pivot_wider(names_from = name, values_from = count) %>%
    mutate(tile = tiles[n, ]$tile,
           ecozone = tiles[n, ]$ecozone,
           index = tiles[n, ]$index)
  if (nrow(lb) > 0) lb
}, mc.cores = 12) %>% bind_rows()
save(tiles_labels, file = 'data/intermid/tiles_labels_256.rda')

class_nm <- c('Cropland', 'Forest', 'Grassland', 'Shrubland', 
              'Wetland', 'Water', 'Urban/Built up', 'Bareland')
tiles_grids_pure <- do.call(rbind, 
                            lapply(class_nm, 
                                   function(nm){
    message(nm)
    # Ensure the enough amount of labels,
    # use different threshold to choose labels,
    # even though the labels might not be perfect.
    # Bareland, the most corrupted, so use 0.4
    # Shrubland and Cropland, use 0.7
    # Grassland, Forest, and water are probably the 
    # easiest classes.
    if (nm %in% c('Bareland', 'Wetland', 'Urban/Built up')) {
      tile_select <- tiles_labels %>% 
        dplyr::select(tile, index, all_of(nm)) %>% 
        filter(!is.na(!!rlang::sym(nm))) %>% 
        filter((!!rlang::sym(nm)) >= 0.6)
    } else {
      tile_select <- tiles_labels %>% 
          dplyr::select(tile, index, all_of(nm)) %>% 
          filter(!is.na(!!rlang::sym(nm))) %>% 
          filter((!!rlang::sym(nm)) >= 0.9)
    }
    data.frame(name = nm, 
               tile = tile_select$tile, 
               index = tile_select$index)
}))

grids_pure_count <- tiles_grids_pure %>% 
  group_by(name) %>% summarize(n = n()) %>% 
  arrange(n)
write.csv(grids_pure_count, 'data/intermid/pure_grids_count.csv',
          row.names = F)
```

```{r, eval=T, echo=F, message=F, warning=F}
grids_pure_count <- read.csv(here('data/intermid/pure_grids_count.csv'),
                             stringsAsFactors = F)

# Based on the table, the first three minor classes 
grids_pure_count %>%
  kbl(caption = "Pure label grids summary") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r g1_sample}
set.seed(123)
grids_common_class <- tiles_grids_pure %>% 
  mutate(name = ifelse(name %in% c('Bareland', 'Wetland', 
                                   'Urban/Built up'), 'Other', 
                       name)) %>% 
  group_by(name) %>% 
  sample_n(500) %>% 
  ungroup()
```

##### Group 2
```{r g2}
# choose cities based on the consensus built-up
nm <- 'Urban/Built up'
cities <- tiles_labels %>% 
        dplyr::select(tile, index, all_of(nm)) %>% 
        filter(!is.na(!!rlang::sym(nm))) %>% 
        filter((!!rlang::sym(nm)) >= 0.2) %>%
  anti_join(grids_common_class, by = c('tile', 'index'))

# choose tiles overlapped with rivers
tiles <- st_read('data/intermid/tiles_grids_ecozones_256.geojson')
rivers <- st_read('data/geoms/gis_osm_waterways_free_1.shp') %>% 
  filter(fclass == 'river')
set.seed(160)
tiles_rivers <- tiles %>% 
  slice(unique(unlist(st_intersects(rivers, tiles)))) %>% 
  anti_join(grids_common_class, by = c('tile', 'index')) %>% 
  anti_join(cities, by = c('tile', 'index')) %>% 
  sample_n(1000 - nrow(cities))
cities_rivers <- rbind(cities %>% dplyr::select(tile, index),
                       tiles_rivers %>% dplyr::select(tile, index) %>% 
                         st_drop_geometry()) %>%
  unique() %>% mutate(name = 'Special')
```

##### Group 3
```{r g3}
g1_g2 <- rbind(grids_common_class, cities_rivers)
tiles <- tiles %>% anti_join(g1_g2, by = c('tile', 'index'))

# Only keep the grids with labels
target_vars <- setdiff(names(tiles_labels), 
                       c('ecozone', 'tile', 'index'))
tiles_full <- tiles_labels %>% 
  mutate(ratio = rowSums(.[target_vars], na.rm = TRUE)) %>% 
  filter(ratio > 0.8)
tiles <- tiles %>% right_join(tiles_full, by = c('tile', 'index'))

set.seed(170)
random_sites <- tiles %>% sample_n(1000) %>% 
  dplyr::select(tile, index) %>% 
  st_drop_geometry() %>% 
  mutate(name = 'random')
```

##### All grids

```{r g123}
grids_selected <- rbind(g1_g2, random_sites)
rm(g1_g2, random_sites, cities, rivers, tiles_rivers, cities_rivers);gc()
```

#### Split training and validate

```{r split_pure}
set.seed(100)
grids_train <- grids_selected %>% 
  group_by(name) %>% sample_frac(0.7) %>% 
  ungroup() %>% dplyr::select(-name) %>% 
  unique() %>% mutate(usage = 'train') %>%
  dplyr::select(tile, index, usage)

grids_valid <- grids_selected %>% 
  anti_join(grids_train, 
            by = c('tile', 'index')) %>% 
  group_by(name) %>% ungroup() %>% 
  dplyr::select(-name) %>% 
  unique() %>% mutate(usage = 'validate') %>%  
  dplyr::select(tile, index, usage)

# In order to set the batch size easily for DL,
# We re-arrange the sites among train and validate
# Train 3510, so 3510 / 30 could be without remainder.
# Validate 1470, so 1470 / 10 could be without remainder.
set.seed(678)
ids <- sample(1:nrow(grids_valid), 
              size = (nrow(grids_valid) - (3510 - nrow(grids_train))) %% 10)
grids_valid <- grids_valid %>% slice(-ids)

set.seed(568)
ids <- sample(1:nrow(grids_valid), size = 3510 - nrow(grids_train))
grids_valid$usage[ids] <- 'train'

catalog_train_val <- rbind(grids_train, grids_valid)

write.csv(catalog_train_val, 
          'data/intermid/catalog_guess_train_val.csv',
          row.names = F)
```

#### Generate labels images
In order to save the resources for deep learning, we re-defined the class index to be continuous. So:

```
id          name
1       Cropland
2       Forest
3       Grassland
4       Shrubland
5       Other (Wetland 5, Urban/Built up 8, Bareland 9)
6       Water
```

```{r labels_img}
library(glue)

# Make the script consistent
tiles <- st_read('data/intermid/tiles_grids_ecozones_256.geojson',
                 quiet = T)
lc_labels <- rast('data/intermid/lc_labels.tif')

plt_path <- '/Volumes/elephant/plt_nicfi'
plt_nms <- list.files(plt_path, full.names = T)
labels_path <- '/Volumes/elephant/labels_guess'

mclapply(unique(catalog_train_val$tile),
         function(tile_nm){
  tiles_each <- catalog_train_val %>% 
    filter(tile == tile_nm)
  lc_labels_each <- crop(lc_labels, tiles %>% 
                           filter(tile == tile_nm) %>% 
                           st_union() %>% 
                           st_buffer(0.01) %>% 
                           st_sf() %>% vect())
  nm <- grep(tile_nm, plt_nms, value = TRUE)[1]
  temp <- rast(nm)[[1]] %>% 
    aggregate(fact = 4096/width_chips)
  vals <- matrix(1:(width_chips^2), 
                 nrow = width_chips, 
                 ncol = width_chips) %>% t()
  vals <- vals[nrow(vals):1, ]
  values(temp) <- vals
  lapply(1:nrow(tiles_each), function(n){
    tile_each <- tiles_each %>% slice(n)
    temp_each <- temp
    index <- tile_each$index
    temp_each[temp_each != index] <- NA
    temp_each <- terra::trim(temp_each) %>% 
      disaggregate(fact = 4096/width_chips)
    label_each <- resample(lc_labels_each, temp_each, method = 'near')
    label_each[label_each %in% c(8, 9)] <- 5
    label_each[label_each %in% c(7, 10)] <- 0
    
    # freq(label_each)[, 2]
    writeRaster(label_each, 
                file.path(labels_path, 
                          glue('labels_{tile_nm}_{index}.tif')),
                wopt = list(datatype = 'INT1U',
                        gdal=c("COMPRESS=LZW")))
    rm(tile_each, temp_each, label_each, index);gc()
  })
  rm(tiles_each, lc_labels_each, nm, temp, vals);gc()
}, mc.cores = 12)
```

## Make catalogs
### Catalog for guess training

```{r catalogs}
library(stringr)
library(parallel)

## PlanetScope quads
plt_quads_rain_season <- list.files('/Volumes/elephant/plt_nicfi',
                        full.names = T, pattern = '2017-12_2018-05') %>% 
  data.frame(dir_gs = .) %>% 
  mutate(tile = str_extract(str_extract(dir_gs, 
                                        '[0-9]+-[0-9]+.tif'), 
                            '[0-9]+-[0-9]+'))
plt_quads_dry_season <- list.files('/Volumes/elephant/plt_nicfi',
                        full.names = T, pattern = '2018-06_2018-11') %>% 
  data.frame(dir_os = .) %>% 
  mutate(tile = str_extract(str_extract(dir_os, 
                                        '[0-9]+-[0-9]+.tif'), 
                            '[0-9]+-[0-9]+'))

plts <- merge(plt_quads_rain_season, plt_quads_dry_season, by = 'tile')
rm(plt_quads_rain_season, plt_quads_dry_season)
## Labels
lc_labels <- list.files('/Volumes/elephant/labels_guess',
                        full.names = T, pattern = '.tif$') %>% 
    data.frame(dir_label = .) %>% 
    mutate(name = str_extract(dir_label, 
                              '[0-9]+-[0-9]+_[0-9]+')) %>% 
    tidyr::separate(name, c('tile', 'index'), '_')
imgs <- merge(lc_labels, plts, by = 'tile')
# rm(plts, lc_labels)

catalog_train_val <- read.csv('data/intermid/catalog_guess_train_val.csv',
                                  stringsAsFactors = F) %>% 
    merge(imgs, by = c('tile', 'index')) %>% 
    mutate(name = paste(tile, index, sep = '-')) %>% 
    dplyr::select(name, usage, dir_gs, dir_os, dir_label)
write.csv(catalog_train_val, 'data/intermid/catalog_DL_guess_train_val.csv',
          row.names = F)
```

### Catalog for guess prediction
We make the prediction on all relevant tiles, and extra 200 tiles in our areas. One thing need to mention is that in order to reduce the impacts of chip boundary and give a better vision for checkers in next stage to manually check the generated labels, we just generate the labels for the central 3840 * 3840 of the original 4096 * 4096.

```{r}
pred_labeled <- catalog_train_val %>% 
  mutate(tile = str_extract(str_extract(dir_gs, 
                                        '[0-9]+-[0-9]+.tif'), 
                            '[0-9]+-[0-9]+')) %>% 
  dplyr::select(tile, dir_gs, dir_os) %>% unique() %>% 
  mutate(with_label = 1)

set.seed(320)
pred_unlabeled <- st_read('data/geoms/tiles_nicfi.geojson') %>% 
  filter(!tile %in% pred_labeled$tile) %>% 
  sample_n(200) %>% 
  merge(plts, by = 'tile') %>% 
  st_drop_geometry() %>% 
  dplyr::select(tile, dir_gs, dir_os) %>% 
  mutate(with_label = 0)

pred_catalog <- rbind(pred_labeled, pred_unlabeled)
rm(pred_labeled, pred_unlabeled)
write.csv(pred_catalog, 'data/intermid/catalog_DL_guess_predict.csv',
          row.names = F)
```

### AWS based (Optional)

Some pre-work before model training:

- Upload files to our AWS S3 bucket.
- Make the data catalog.

#### Upload imagery
Upload the images and labels to AWS S3 bucket.

```{r upload}
library(aws.s3)
library(parallel)

## PlanetScope quads
plt_quads_rain_season <- list.files('/Volumes/elephant/plt_nicfi',
                        full.names = T, pattern = '2017-12_2018-05')
plt_quads_dry_season <- list.files('/Volumes/elephant/plt_nicfi',
                        full.names = T, pattern = '2018-06_2018-11')
## Labels
lc_labels <- list.files('/Volumes/elephant/labels_guess',
                        full.names = T, pattern = '.tif$')

## Upload function
s3_upload <- function(fnames, prefix, bucket){
    mclapply(fnames, function(src_path){
    put_object(file = src_path, 
           object = file.path(prefix, 
                              basename(src_path)), 
           bucket = bucket)
}, mc.cores = 6)}

## Upload
s3_upload(plt_quads_rain_season, 
          "tz_dl_data/planet_quads/rain", 
          'activemapper')
s3_upload(plt_quads_dry_season, 
          "tz_dl_data/planet_quads/dry", 
          'activemapper')
s3_upload(lc_labels, 
          "tz_dl_data/lc_labels", 
          'activemapper')
```

#### Make catalog

```{r catalog}
library(dplyr)
library(stringr)
library(tidyr)
library(aws.s3)

# Rain season
items <- get_bucket(bucket = 'activemapper', 
                    prefix = 'tz_dl_data/planet_quads/rain', 
                    max = Inf)
keys <- lapply(c(2:length(items)), function(i) {
    basename(items[[i]]$Key)
})
plts_rain <- unlist(keys) %>% 
    paste0('s3://activemapper/tz_dl_data/planet_quads/rain/', .) %>% 
    data.frame(dir_gs = .) %>% 
    mutate(tile = str_extract(str_extract(dir_gs, 
                                          '[0-9]+-[0-9]+.tif'), 
                              '[0-9]+-[0-9]+'))

# Dry season
items <- get_bucket(bucket = 'activemapper', 
                    prefix = 'tz_dl_data/planet_quads/dry', 
                    max = Inf)
keys <- lapply(c(2:length(items)), function(i) {
    basename(items[[i]]$Key)
})
plts_dry <- unlist(keys) %>% 
    paste0('s3://activemapper/tz_dl_data/planet_quads/dry/', .) %>% 
    data.frame(dir_os = .) %>% 
    mutate(tile = str_extract(str_extract(dir_os, 
                                          '[0-9]+-[0-9]+.tif'), 
                              '[0-9]+-[0-9]+'))
plts <- merge(plts_rain, plts_dry, by = 'tile')
rm(plts_rain, plts_dry)

# Labels
items <- get_bucket(bucket = 'activemapper', 
                    prefix = 'tz_dl_data/lc_labels', 
                    max = Inf)
keys <- lapply(c(2:length(items)), function(i) {
    basename(items[[i]]$Key)
})
lc_labels <- unlist(keys) %>% 
    paste0('s3://activemapper/tz_dl_data/lc_labels/', .) %>% 
    data.frame(dir_label = .) %>% 
    mutate(name = str_extract(dir_label, 
                              '[0-9]+-[0-9]+_[0-9]+')) %>% 
    tidyr::separate(name, c('tile', 'index'), '_')
imgs <- merge(lc_labels, plts, by = 'tile')
rm(plts, lc_labels)

catalog_train_val_test <- read.csv('data/intermid/catalog_guess_train_val.csv',
                                  stringsAsFactors = F) %>% 
    merge(imgs, by = c('tile', 'index')) %>% 
    mutate(name = paste(tile, index, sep = '-')) %>% 
    dplyr::select(name, usage, dir_gs, dir_os, dir_label)
write.csv(catalog_train_val_test, 'data/intermid/catalog_DL_guess_train_val.csv',
          row.names = F)
put_object(file = 'data/intermid/catalog_DL_guess_train_val.csv', 
           object = 'tz_dl_data/catalogs/catalog_DL_guess_train_val.csv', 
           bucket = 'activemapper')
```
