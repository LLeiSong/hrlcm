---
title: "Introduction of using AWS instance to run experiments"
author: "Lei Song"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown:
    highlight: pygments
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Set up GPU environment
### AWS based method
#### Launch AWS instance
If you decide to use AWS instance, here are some basic steps.
The information of instance that we used:

- AMI ID: `ami-0643df3b347740189`, which is our customized deep learning AWS instance. Or this one `ami-005050e76c11b3b16`, which includes datasets already.
- Instance type: a few instance could use based on different needs.

  - `g3.8xlarge`, 2 GPU, 16G GPU memory, 32 CPUs, 244 GB CPU memory, $0.684/hr for SPOT instance.
  - `g3.16xlarge`, 4 GPU, 32G GPU memory, 64 CPUs, 488 GB CPU memory, $1.368/hr for SPOT instance.
  - `g4dn.8xlarge`, 1 GPU, 16G GPU memory, 32 CPUs, 128 GB CPU memory, $0.6528/hr for SPOT instance.
  - `g4dn.12xlarge`, 4 GPUs, 64G GPU memory, 48 CPUs, 192 GB CPU memory, $1.1819/hr for SPOT instance.
  - `g5.xlarge`, 1 GPU, 24 GPU memory, 4 CPUs, 16 GB CPU memory (on-demand $1.212/hr, spot 0.3018/hr).
  - `g5.2xlarge`, 1 GPU, 24 GPU memory, 8 CPUs, 32 GB CPU memory (on-demand $1.624/hr, spot 0.3636/hr).
  - `g5.4xlarge`, 1 GPU, 24 GPU memory, 16 CPUs, 64 GB CPU memory (on-demand $2.448/hr, spot 0.4872/hr).
  - `g5.8xlarge`, 1 GPU, 24 GPU memory, 32 CPUs, 128 GB CPU memory (on-demand $4.096/hr, spot 0.967/hr).
  - `g5.12xlarge`, 4 GPUs, 96 GPU memory, 48 CPUs, 192 GB CPU memory (on-demand $5.672/hr, 1.7016/hr).
    
- Security group: `labeller-security`.

Before run this script, do the following steps. Just need step one for Linux machine. Didn't test Windows machine.

1. `brew install awscli`, then `aws configure` to configure AWS credential.
2. `brew install jq`
3. `brew install coreutils`. to use gdata on macOS.

Then:
```
cd tools
chmod 777 create_spot_instance.sh
create_spot_instance.sh <ami_id> <instance_type> <security_group_id> <new_instance_name> <spot_type> <valid_until> <key_name> <volume_size>
```
For example:
```
cd tools
chmod 777 create_spot_instance.sh
create_spot_instance.sh ami-0643df3b347740189 g3.16xlarge sg-0a8bbc91697d6a76b lc_dl_tz persistent 2021-01-22T23:00:00 lsong-keypair 200
```

#### Instance setting
After successfully launched the instance, then it is ready to get into the instance to work.

The most essential work are arguably to clone project and set jupyter notebook:

- STEP 1: Set up GitHub token (because the repo is private). More details could be found in this [tutorial](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).

  ```
  ssh-keygen -t ed25519 -C "your_email@example.com"
  eval "$(ssh-agent -s)"
  ssh-add ~/.ssh/id_ed25519
  vim ~/.ssh/id_ed25519.pub
  ```
  Copy out the ssh key, and type `:q` to exit, and then go to GitHub page to add into the records.
    
- STEP 2: Clone the repo into instance:
  
  ```
  git clone git@github.com:LLeiSong/hrlcm.git
  ```
  
  If necessary, use `git checkout` to switch branch and use `git pull` to grab the updates.
  
- STEP 3: Set up jupyter notebook

  ```
  jupyter notebook password
  ```
  to set a log in password for jupyter notebook, then
  ```
  jupyter notebook
  ```
Replace the private IP with the public IP to open it in browser. If you can log in without issues, then it is ready to go.
  
### Other cloud computing platform

The users could set up the deep learning environment with similar steps used on AWS. For example, Colab can be used as long as the GPU power is enough for your computation.
  
## Run experiments

### Parse dataset

```
cd path/of/hrlcm
./hrlcm/utils/parse_dataset.sh path/to/save
```

### Run models

```
cd path/of/hrlcm
python hrlcm/fit_step_lr.py --arg arg_value
```

Check arguments:
```
python hrlcm/fit_step_lr.py --help
```

For example:
```
python hrlcm/fit_step_lr.py --exp_name unet_one_tile --num_workers 32 --gpu_devices '0,1,2,3'
```
Or
```
python hrlcm/fit_compose_lr.py --exp_name unet_all_norm_200 --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.00001 --max_lr 0.001 --save_freq 20 --epochs 201
```

### Tensorboard

If download log file to local machine, then it is easy to check the training loss curves.

`tensorboard --logdir='./tensorboard_dirs' --port=16007`

Do the following steps to use the tensorboard on a remote server (reference: https://blog.yyliu.net/remote-tensorboard/):

1. You need to start SSH with transfer the remote serverâ€™s port to your local PC:

    (on your local PC) `ssh -L 16007:127.0.0.1:16007 username@server_ip`

2. Then you may start the tensorboard on the remote server with the specific port we want to transfer:

    (on the remote server) `tensorboard --logdir='./tensorboard_dirs' --port=16007`
    
### Hyperparameter tunning
#### Network parameter

Before official running, test a bit about the largest values of `num_worker`, `train_batch_size`, `val_batch_size` that could be used. For `g5.12xlarge`, these values can be:

- `num_worker`:48 (CPU cores)
- `train_batch_size`: 32 (could be a bit bigger, but it is recommended to use $2^n$)
- `val_batch_size`: 32

#### Learning rate
Also, test the optimal learning rate and scheduler to use:

- 0.001 is a good start point.
- 0.00001 is a good end point with middle value of 0.0001.

`final_lr` for `optim.AdaBound`:

- 0.01 still works fine. The default value is robust for many cases.

#### Augmentation

- Random flip (always use)
- Random scale
- Random center rotate
- Normalization: Dataset based standardization. *Among dataset-based standardization, image-based standardization, image-based normalization, and no such transformation, dataset-based standardization works the best because the U-Net learned much faster.*

Experiments indicate using all of them can improve model performance.

#### Loss functions

No luck on using different loss function. The most effective loss function is still the balanced cross entropy loss function.

According to the experiments, balanced cross entropy loss performs well. In summary (based on my knowledge), changing loss function is not an efficient and effective way to increase the model performance in this case. This might because the modeling task here does not have special characteristics that need special loss functions to deal with. For example, some loss functions were created to detect vessel-like objects. Instead, the problems here are mainly the noisy labels and not enough representative features (satellite image bands).

#### Loss weights

According to some previous experiments, the accuracy of cropland is relatively low. One reason is that the farming condition over Tanzania is complex and thus hard to model. Another reason is the validation of this class has relatively low quality as well. There are limited things we could do for these reasons. But we can let the model to consider more about cropland when training. The strategy can be increasing the loss weight of cropland class. However, the effect is not conclusive.

Hum... Using a bigger weight for cropland in loss calculation increased the overall accuracy a bit. However, the cost is the overestimation of cropland over sparse vegetation area.

#### Freeze layers

Sometimes, people freeze some layers, only update weights of last few layers. The parameters for U-Net used in this project are: 

1 * 8 (initial) + 4 * 8 (down) + 4 * 8 (up) + 2 = 74 layers

- 40: freeze all down layers
- 56: freeze all down and the first two up blocks
- 64: freeze all down and the first three up blocks
- 72: freeze all down and up blocks, just update the last two layers

#### U-Net structure

Using `ConvTranspose2d` instead of `Upsample` could reduce the overestimation issue of built-up areas. Because upsampling just uses bilinear sampling method, which could smooth the whole image. However, built-up objects are usually tiny. Thus, they are affected by bilinear upsampling a lot.

However, there are also cons of `ConvTranspose2d`. Using `ConvTranspose2d` loses the advantage of bilinear smoothing. Thus, at the boundary of an object, it might be some salt noises. If post-segmentation pruning is applied, this is not a big issue.

Another issue is that because using `ConvTranspose2d` means add another learnable parameter layer, it slightly increases the need of computational resources. It really depends if this is an issue.

Thus, whether using `ConvTranspose2d` or not depends what you want. 

#### "Hierachical" training

When training a full model using the whole dataset, the main problem is cropland. One reason is that
cropland highly relies on local climate and cultivating conditions and Tanzania is a large country. To reduce this impact, we could shrink the study area gradually into each agro-ecological zone.

- Modeling structure:

```
1, 2, 4, 5 -> 1, 2, 5 -> 1, 2 -> 1
                      -> 2, 5 -> 2
                              -> 5
2, 3, 4, 5 -> 2, 3, 4 -> 2, 3 -> 3
                      -> 2, 4 -> 4
```

Zone 1, 2, 3, 4, 5

Comment out line 211 and add `learning_rates = [args.max_lr] * args.epochs` to use a fixed learning rate 0.001 to train.

```
python hrlcm/fit_compose_lr.py --exp_name unet_full_startup_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --max_lr 0.001 --save_freq 20 --epochs 81
```

Then, discard the changes above.

Zone 1, 2, 4, 5
```
python hrlcm/fit_compose_lr.py --exp_name unet_z1245_120epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.00001 --max_lr 0.001 --save_freq 20 --epochs 121 --resume 'results/dl/unet_full_startup_80epochs/checkpoints/final.pth'
```

Comment out line 249 in `fit_compose_lr.py`, and modify `lr_scheduler.py` correspondingly. The learning rate scheduler is saved as a pickle as well.

Zone 1, 2, 5
```
python hrlcm/fit_compose_lr.py --exp_name unet_z125_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.00001 --max_lr 0.0001 --save_freq 10 --epochs 81 --resume 'results/dl/unet_z1245_120epochs/checkpoints/final.pth'

# Fine tune using a smaller learning rate
python hrlcm/fit_compose_lr.py --exp_name unet_z125_ft_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.000001 --max_lr 0.00001 --save_freq 10 --epochs 81 --resume 'results/dl/unet_z125_80epochs/checkpoints/final.pth'
```

Discard the changes above.

Zone 2, 3, 4, 5
```
python hrlcm/fit_compose_lr.py --exp_name unet_z2345_120epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.00001 --max_lr 0.001 --save_freq 20 --epochs 121 --resume 'results/dl/unet_full_startup_80epochs/checkpoints/final.pth'
```

Do the same as zone 1, 2, 5.

Zone 2, 3, 4
```
python hrlcm/fit_compose_lr.py --exp_name unet_z234_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.00001 --max_lr 0.0001 --save_freq 20 --epochs 81 --resume 'results/dl/unet_z2345_120epochs/checkpoints/final.pth'

# Fine tune with a smaller learning rate
python hrlcm/fit_compose_lr.py --exp_name unet_z234_ft_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.000001 --max_lr 0.00001 --save_freq 10 --epochs 81 --resume 'results/dl/unet_z234_80epochs/checkpoints/final.pth'
```

**[NOTE:]** _I tried multiple ways, which includes adding more training labels, using different types of loss functions (particularly ones to deal with class imbalance), and applying different model structure. No luck to get a reasonable balanced results if shrink the training area further into single zones. So I decided to stop here to use three zones as a group for the final result._

_But if you like, or I get the right method to do further, go further to finish the following steps._

_Zone 1_
```
# "Shrink" the model to zone1
python hrlcm/fit_compose_lr.py --exp_name unet_z125_ew_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.00001 --max_lr 0.0001 --save_freq 10 --epochs 81 --resume 'results/dl/unet_z125_80epochs/checkpoints/final.pth'

# "Fine-tune" the zone1 model
python hrlcm/fit_compose_lr.py --exp_name unet_z1_ft_72fz_cl_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --max_lr 0.00000001 --save_freq 10 --epochs 81 --resume 'results/dl/unet_z1_ew80epochs/checkpoints/final.pth'
```

_Zone 2, 5 (For zone 2 & 5)_
```
python hrlcm/fit_compose_lr.py --exp_name unet_z25_ew_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.00001 --max_lr 0.0001 --save_freq 20 --epochs 81 --resume 'results/dl/unet_z125_80epochs/checkpoints/final.pth'
```

_Zone 2, 3 (For zone 3)_
```
python hrlcm/fit_compose_lr.py --exp_name unet_z23_12bs_4augs_ft_fz56_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.0000001 --max_lr 0.00001 --save_freq 20 --epochs 81 --resume 'results/dl/unet_z234_12bs_4augs_ft_80epochs/checkpoints/final.pth'
```

_Zone 2, 4 (For zone 4)_
```
python hrlcm/fit_compose_lr.py --exp_name unet_z24_12bs_4augs_ft_fz56_80epochs --num_workers 48 --gpu_devices '0,1,2,3' --min_lr 0.0000001 --max_lr 0.00001 --save_freq 20 --epochs 81 --resume 'results/dl/unet_z234_12bs_4augs_ft_80epochs/checkpoints/final.pth'
```

### Prediction

Doing prediction is simple. Applying a model into its zones.

```
cd path/of/hrlcm
python hrlcm/predict.py --arg arg_value
```
Check arguments:
```
python hrlcm/predict.py --help
```

For example:
```
python hrlcm/predict.py --args_path 'results/dl/unet_z125_ft_80epochs/checkpoints/args.pkl' --checkpoint_file 'results/dl/unet_z125_ft_80epochs/checkpoints/final.pth' --fname_predict 'dl_catalog_predict.csv' --data_dir 'results/tanzania' --out_dir 'results/prediction' --stats_dir 'results/tanzania/norm_stats' --num_workers 48 --batch_size 32 --gpu_devices '0,1,2,3'

aws s3 cp --recursive $HOME/hrlcm/results/prediction s3://activemapper/leisong/prediction
```

Steps in production:

```

python hrlcm/predict.py --args_path 'results/dl/unet_z125_ft_80epochs/checkpoints/args.pkl' --checkpoint_file 'results/dl/unet_z125_ft_80epochs/checkpoints/final.pth' --fname_predict 'dl_catalog_predict_zone1_1.csv' --data_dir 'results/tanzania' --out_dir 'results/prediction' --stats_dir 'results/tanzania/norm_stats' --num_workers 48 --batch_size 32 --gpu_devices '0,1,2,3'

aws s3 cp --recursive $HOME/hrlcm/results/prediction s3://activemapper/leisong/prediction/zone1

rm -rf results/prediction
```

```{r}
# Get imgs in S3
library(aws.s3)
library(dplyr)
library(parallel)
items <- get_bucket(bucket = "activemapper", 
                    prefix = "leisong/predict", 
                    max = Inf)
keys <- unlist(lapply(c(2: length(items)), function(i) {
    gsub("leisong/", "", items[[i]]$Key)
}))

catalog_pred <- read.csv("results/tanzania/dl_catalog_predict.csv", 
                         stringsAsFactors = FALSE)
fns <- catalog_pred$img %>% unique()

fns <- setdiff(fns, keys)
pth_from <- '/Volumes/elephant'
pth_to <- 's3://activemapper/leisong'
mclapply(fns, function(tile_pth) {
    print(tile_pth)
    cmd <- sprintf('aws s3 cp %s/%s %s/%s',
                   pth_from, tile_pth, pth_to, tile_pth)
    system(cmd, intern = TRUE)
}, mc.cores = 10)
```


### Evaluation

```
cd path/of/hrlcm
python hrlcm/evaluate.py --arg arg_value
```
Check arguments:
```
python hrlcm/evaluate.py --help
```

For example:
```
python hrlcm/evaluate.py --args_path 'results/dl/unet_z12_12bs_4augs_ft_fz56_80epochs/checkpoints/args.pkl' --checkpoint_file 'results/dl/unet_z12_12bs_4augs_ft_fz56_80epochs/checkpoints/final.pth' --data_dir 'results/tanzania' --out_dir 'results/evaluation' --stats_dir 'results/tanzania/norm_stats' --num_workers 48 --batch_size 32 --gpu_devices '0,1,2,3'

scp -r ubuntu@ec2-18-191-228-234.us-east-2.compute.amazonaws.com:~/hrlcm/results/evaluation /Users/pinot/Dropbox/research/hrlcm/results/dl/unet_score08_hardiness2_epoch200
```

#### Post-cleaning

- Cropland/Water: Object fixing
- Grassland/Shrubland/Forest/Build-up/Bareland: No fix

## Other useful commands

### Check disk space and clean trash

```bash
df -H
cd ~/.local/share/Trash/
rm -rf *
```
### Check the RAM usage in M

```bash
free -m
```

### Copy files from/to S3

To:

```
aws s3 cp --recursive $HOME/hrlcm/results/dl s3://activemapper/leisong/model

while true; do aws s3 cp --recursive $HOME/hrlcm/results/dl/unet_full_startup_80epochs s3://activemapper/leisong/model/unet_full_startup_80epochs; sleep 50m; done

aws s3 cp --recursive $HOME/hrlcm/results/prediction_zone1_125 s3://activemapper/leisong/prediction/
```

From:

```
aws s3 cp --recursive s3://activemapper/leisong/unet_full_12bs_4augs_120epochs/ $HOME/hrlcm/results/dl/unet_full_12bs_4augs_120epochs
```